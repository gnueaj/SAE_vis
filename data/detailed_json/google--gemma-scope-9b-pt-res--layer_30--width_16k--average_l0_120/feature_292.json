{
  "feature_id": 292,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1070",
      "text": "Prepositions and articles often precede nouns, and sometimes adjectives or adverbs, that are crucial for understanding the context of a sentence, often in scientific or technical writing.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_246",
      "text": "Prepositions and articles (such as \\\"of\\\", \\\"in\\\", \\\"the\\\", \\\"to\\\", \\\"and\\\") frequently appear in context with scientific terminology, often forming part of compound terms or modifying nouns in technical descriptions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1893",
      "text": "The highlighted tokens are mainly short function words and domain‑specific nouns that form common multi‑word expressions (e.g., prepositional phrases, article‑noun pairs, and technical terms) which the model deems salient for contextual understanding.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1070",
        "exp_246"
      ],
      "cosine_similarity": 0.9477681043613736,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1070",
        "exp_1893"
      ],
      "cosine_similarity": 0.8773648342305335,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_246",
        "exp_1893"
      ],
      "cosine_similarity": 0.8775507145504752,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6842105263157895,
      "score_detection": 0.55,
      "score_simulation": 0.10712489728777706,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.6,
      "score_simulation": 0.16299308828870465,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.575,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.71,
      "score_detection": 0.63,
      "score_simulation": 0.12799224964976963,
      "score_embedding": 0.4396
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.63,
      "score_detection": 0.61,
      "score_simulation": -0.023555807749923913,
      "score_embedding": 0.3516
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}