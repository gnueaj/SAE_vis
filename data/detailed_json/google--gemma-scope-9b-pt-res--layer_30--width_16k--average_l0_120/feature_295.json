{
  "feature_id": 295,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1072",
      "text": "Punctuation marks, often used to denote separation, grouping, or special formatting in text, such as periods, commas, parentheses, and colons.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_248",
      "text": "The token sequences often represent punctuation marks, delimiters, or syntactic elements used in code, markup, or formatting, such as parentheses, brackets, dots, quotes, and special symbols, which are frequently used to structure or separate components in technical or structured text.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1895",
      "text": "The highlighted fragments are the minimal substrings that carry the semantic or syntactic core of a word, identifier, or symbol, often split across larger tokens, and they serve as the key units for the modelâ€™s activation.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1072",
        "exp_248"
      ],
      "cosine_similarity": 0.9068533025374013,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1072",
        "exp_1895"
      ],
      "cosine_similarity": 0.8105414745196236,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_248",
        "exp_1895"
      ],
      "cosine_similarity": 0.8532750026697545,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.58,
      "score_detection": 0.5,
      "score_simulation": 0.057648636012219666,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5333333333333333,
      "score_detection": 0.4842105263157895,
      "score_simulation": 0.13249798108070168,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.73,
      "score_detection": 0.51,
      "score_simulation": 0.12220789499061595,
      "score_embedding": 0.4264
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.71,
      "score_simulation": -0.03975075256617544,
      "score_embedding": 0.5204
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}