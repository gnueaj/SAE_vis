{
  "feature_id": 449,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1192",
      "text": "Verbs or nouns expressing necessity, desire, or importance, often in the context of human needs, wants, or obligations.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_368",
      "text": "The word \\\"need\\\" and its variants (e.g., \\\"needed\\\", \\\"want\\\", \\\"must\\\", \\\"should\\\") frequently appear in contexts expressing necessity, desire, or obligation, often preceding or following key actions or conditions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2015",
      "text": "The highlighted tokens are the core content words that carry the main semantic load of each sentenceâ€”typically nouns, verbs, or adjectives that serve as the key subject, object, or predicate. They are the words the model deems most important for understanding or generating the sentence, and they are not ordinary function words.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1192",
        "exp_368"
      ],
      "cosine_distance": 0.06706063025959408,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1192",
        "exp_2015"
      ],
      "cosine_distance": 0.15177569461580487,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_368",
        "exp_2015"
      ],
      "cosine_distance": 0.18749254179468933,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.83,
      "score_detection": 0.59,
      "score_simulation": 0.5293514966164719,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.78,
      "score_detection": 0.7571428571428571,
      "score_simulation": 0.49820973269391833,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.75,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.87,
      "score_detection": 0.7,
      "score_simulation": 0.2303906284882011,
      "score_embedding": 0.7908000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.78,
      "score_detection": 0.72,
      "score_simulation": 0.41949451868413157,
      "score_embedding": 0.7979999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}