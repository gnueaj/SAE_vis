{
  "feature_id": 666,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1375",
      "text": "Prepositions indicating relationships such as possession, agency, or direction, often used in formal or written contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_551",
      "text": "Prepositions such as \\\"to\\\", \\\"of\\\", \\\"by\\\", \\\"for\\\", \\\"with\\\", and \\\"in\\\" frequently appear in contexts involving relationships of direction, possession, agency, or location, often preceding or following key nouns in syntactic constructions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2198",
      "text": "The highlighted tokens are short prepositions or articles that serve as grammatical connectors, linking nouns, phrases, or clauses and indicating relationships such as possession, direction, or association.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1375",
        "exp_551"
      ],
      "cosine_similarity": 0.9317586923642719,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1375",
        "exp_2198"
      ],
      "cosine_similarity": 0.8970914720185852,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_551",
        "exp_2198"
      ],
      "cosine_similarity": 0.8922278576154113,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.75,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.41625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.76,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.3476000000000001
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.95,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.353125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.825,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.353125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.925,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.3476000000000001
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.725,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.41625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.81,
      "score_detection": 0.36,
      "score_simulation": null,
      "score_embedding": 0.3476000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.83,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.41625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.775,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.353125
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}