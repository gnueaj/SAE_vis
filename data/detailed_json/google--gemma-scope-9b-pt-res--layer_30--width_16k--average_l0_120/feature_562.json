{
  "feature_id": 562,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1286",
      "text": "Prepositions, articles, and conjunctions, often in combination with nouns or other parts of speech, that provide context and structure to sentences.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_462",
      "text": "The model attends to sequences of tokens that form meaningful linguistic units such as proper nouns, technical terms, idiomatic expressions, or syntactic structures, often involving compound words, punctuation, or specific grammatical patterns like prepositional phrases and comparative suffixes.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2109",
      "text": "Important tokens are usually nouns, adjectives, or prepositions that belong to a phrase conveying a concrete idea—often a location, object, or comparative form—and they frequently appear as part of a noun or prepositional phrase.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1286",
        "exp_462"
      ],
      "cosine_similarity": 0.8509510010210777,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1286",
        "exp_2109"
      ],
      "cosine_similarity": 0.8770810265102618,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_462",
        "exp_2109"
      ],
      "cosine_similarity": 0.8759172423315751,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.6571428571428571,
      "score_simulation": null,
      "score_embedding": 0.46125000000000005
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.64,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.5272000000000001
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6666666666666666,
      "score_detection": 0.775,
      "score_simulation": null,
      "score_embedding": 0.5331250000000001
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.775,
      "score_simulation": null,
      "score_embedding": 0.5331250000000001
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.5272000000000001
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.46125000000000005
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7052631578947368,
      "score_detection": 0.7894736842105263,
      "score_simulation": null,
      "score_embedding": 0.5272000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.72,
      "score_detection": 0.84,
      "score_simulation": null,
      "score_embedding": 0.46125000000000005
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": 0.5331250000000001
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.2116082532259841,
      "z_score_fuzz": -0.45251014295031816,
      "z_score_detection": 0.808733722676999
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.15348220489155712,
      "z_score_fuzz": 0.06251884610062552,
      "z_score_detection": 0.9077976943440385
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.186282219513944,
      "z_score_fuzz": -0.11717974197945479,
      "z_score_detection": 1.5759956204640895
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.04820510883356558
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.37459958177874036
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.5483660326661929
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}