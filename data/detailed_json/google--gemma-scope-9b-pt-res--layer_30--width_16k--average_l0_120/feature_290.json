{
  "feature_id": 290,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1068",
      "text": "Special characters, abbreviations, and short words or phrases often used in technical, scientific, or programming contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_244",
      "text": "Short, often capitalized or hyphenated sequences of characters (e.g., acronyms, code identifiers, chemical symbols, or technical abbreviations) frequently appear in technical or scientific text, especially within code, mathematical notation, or specialized domains, and are typically highlighted by high activation values due to their structural or semantic specificity.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1891",
      "text": "The highlighted fragments are the smallest contiguous pieces that together form a meaningful unitâ€”idioms, noun phrases, technical terms, or code snippets. They often include suffixes or prefixes that modify a base word or are part of a larger token split across delimiters, and they carry the core semantic content of the surrounding context.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1068",
        "exp_244"
      ],
      "cosine_similarity": 0.9206410751295352,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1068",
        "exp_1891"
      ],
      "cosine_similarity": 0.8605204264940248,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_244",
        "exp_1891"
      ],
      "cosine_similarity": 0.8728358202123387,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5714285714285714,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.3375
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.44,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.418
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6666666666666666,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.343125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.575,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.343125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.418
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.3375
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.418
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.54,
      "score_detection": 0.46,
      "score_simulation": null,
      "score_embedding": 0.3375
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.343125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.8966718497695124,
      "z_score_fuzz": -0.3006316009521512,
      "z_score_detection": -0.2142821410391231
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.4510345203007933,
      "z_score_fuzz": -0.8068934076127078,
      "z_score_detection": -0.40150073047736795
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.8655325953811702,
      "z_score_fuzz": -0.2314969241286124,
      "z_score_detection": -0.35469608311780654
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.4705285305869289
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.5531428861302897
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.4839085342091964
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}