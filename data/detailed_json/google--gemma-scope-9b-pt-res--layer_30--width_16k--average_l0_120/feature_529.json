{
  "feature_id": 529,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1256",
      "text": "Code snippets from various programming languages, including C, C++, and others, often containing function definitions, conditional statements, and variable declarations.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_432",
      "text": "The token sequences often represent syntactic or structural elements in code, such as delimiters, keywords, and identifiers, with high activation on punctuation, braces, parentheses, and identifiers that denote code structure or control flow.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2079",
      "text": "The highlighted tokens are the core semantic units of each fragment—idiomatic phrases in prose and key identifiers, operators, or literals in code—often appearing as contiguous sequences that carry the main meaning of the sentence or statement.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1256",
        "exp_432"
      ],
      "cosine_similarity": 0.8569597344931904,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1256",
        "exp_2079"
      ],
      "cosine_similarity": 0.8280250627743432,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_432",
        "exp_2079"
      ],
      "cosine_similarity": 0.8923957904230917,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5333333333333333,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.59125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.5843999999999999
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5714285714285714,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.6825
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.6825
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.5843999999999999
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.59125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.52,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.5843999999999999
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.46,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.59125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.45,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.6825
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.5080545148601465,
      "z_score_fuzz": -1.1422238085835716,
      "z_score_detection": -0.5536158343959412
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.47013382284945354,
      "z_score_fuzz": -1.0812546447706872,
      "z_score_detection": -0.6589262909549534
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.0132024193821416,
      "z_score_fuzz": -1.1351470306410045,
      "z_score_detection": -0.47170770151670893
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.39592837603978875
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.42334903762539566
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.19788410425852396
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}