{
  "feature_id": 891,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1556",
      "text": "Specialized programming terms, symbols, and syntax from various programming languages.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_732",
      "text": "The text contains sequences of tokens that represent programming language syntax, identifiers, or code constructs, often involving special characters, punctuation, and capitalized or camel-cased identifiers, with activations concentrated on specific syntactic elements like keywords, operators, and identifiers within code blocks.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2379",
      "text": "The highlighted segments consistently capture the core lexical units—words, multi‑word idioms, or code identifiers—that convey the main idea or function in each snippet, with the accompanying numeric scores reflecting their relative importance.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1556",
        "exp_732"
      ],
      "cosine_similarity": 0.8872762313560749,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1556",
        "exp_2379"
      ],
      "cosine_similarity": 0.8256545570366378,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_732",
        "exp_2379"
      ],
      "cosine_similarity": 0.863276480929605,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.525,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": 0.255625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.48,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.288
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.5714285714285714,
      "score_simulation": null,
      "score_embedding": 0.27625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.27625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45714285714285713,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.288
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.255625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.39,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.288
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.325,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.255625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.35,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.27625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.3499209969776045,
      "z_score_fuzz": -1.6985674283761407,
      "z_score_detection": -1.2322832211095773
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.170697288386924,
      "z_score_fuzz": -1.4078751651968533,
      "z_score_detection": -0.9514553369522105
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.2357437308870163,
      "z_score_fuzz": -1.0698229265557713,
      "z_score_detection": -0.7725947202567447
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.4269238821544408
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.1766759301786625
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -1.0260537925665107
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}