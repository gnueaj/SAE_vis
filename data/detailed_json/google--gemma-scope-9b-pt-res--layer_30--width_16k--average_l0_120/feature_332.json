{
  "feature_id": 332,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1103",
      "text": "Function words and common grammatical elements, including articles, conjunctions, prepositions, auxiliary verbs, and modal verbs, often used to connect clauses or phrases, or to indicate relationships between entities or actions.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_279",
      "text": "Common function words and grammatical particles used in conditional, comparative, or contrastive constructions, often appearing in syntactic contexts involving negation, comparison, or logical relationships.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1926",
      "text": "The highlighted tokens are typically short, high‑frequency function words or morphological markers that signal grammatical structure or common collocations, such as “the”, “is”, “or”, “but”, “be”, “of”, “and”, “return”, “success”, or comparative suffixes like “‑er”. These words often serve as anchors in idiomatic phrases or grammatical constructions, making them salient in the model’s activations.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1103",
        "exp_279"
      ],
      "cosine_similarity": 0.8914790935922535,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1103",
        "exp_1926"
      ],
      "cosine_similarity": 0.8685842323966827,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_279",
        "exp_1926"
      ],
      "cosine_similarity": 0.8569051990917175,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.4857142857142857,
      "score_simulation": null,
      "score_embedding": 0.805
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.52,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.6688
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7333333333333333,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.7287500000000001
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.7287500000000001
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.42857142857142855,
      "score_simulation": null,
      "score_embedding": 0.6688
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.805
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.6688
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.59,
      "score_detection": 0.58,
      "score_simulation": null,
      "score_embedding": 0.805
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.725,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.7287500000000001
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.6913461816171498,
      "z_score_fuzz": -0.5782590433143918,
      "z_score_detection": -0.14240357545122545
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.937361035360757,
      "z_score_fuzz": -0.509668734024897,
      "z_score_detection": -0.6973729655717361
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.2692362887974005,
      "z_score_fuzz": 0.14956034970191373,
      "z_score_detection": -0.8812483659128687
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.3235611876171775
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.08989355474529204
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.17918275752881518
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}