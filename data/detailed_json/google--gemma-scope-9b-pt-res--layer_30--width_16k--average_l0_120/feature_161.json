{
  "feature_id": 161,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_959",
      "text": "Special characters and symbols used to denote parameters, return types, and other elements in programming documentation and code.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_135",
      "text": "The token sequences often represent code syntax elements such as parameter declarations, return types, and annotations in programming languages, with emphasis on identifiers, type names, and structural markers like brackets, colons, and special symbols.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1783",
      "text": "The highlighted words are code identifiers, parameter names, and language keywords that carry semantic weight in programming snippets, often surrounded by delimiters and preceded by whitespace or punctuation.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_959",
        "exp_135"
      ],
      "cosine_similarity": 0.9135170188141586,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_959",
        "exp_1783"
      ],
      "cosine_similarity": 0.892836031124393,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_135",
        "exp_1783"
      ],
      "cosine_similarity": 0.9196759119684271,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.44125000000000003
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.46,
      "score_detection": 0.52,
      "score_simulation": null,
      "score_embedding": 0.4576
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.460625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.325,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.460625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.3142857142857143,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.4576
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.25,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": 0.44125000000000003
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.47,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.4576
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.41,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.44125000000000003
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.425,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.460625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.322325602162312,
      "z_score_fuzz": -1.9043383562446252,
      "z_score_detection": -0.8227425567134176
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.23181416940686414,
      "z_score_fuzz": -1.5973150670440293,
      "z_score_detection": -0.3078914357582451
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.21506817038024456,
      "z_score_fuzz": -1.6414088373015612,
      "z_score_detection": -0.764236747513966
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.0164688383734515
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.7123402240697128
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.873571251731924
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}