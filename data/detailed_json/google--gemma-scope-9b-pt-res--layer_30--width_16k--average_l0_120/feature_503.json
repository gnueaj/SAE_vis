{
  "feature_id": 503,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1237",
      "text": "Prefixes or words that are part of a compound word or phrase, often indicating a relationship, direction, or characteristic.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_413",
      "text": "Commonly activated tokens include suffixes like \\\"er\\\", \\\"ing\\\", \\\"ly\\\", and \\\"able\\\", as well as function words like \\\"the\\\", \\\"to\\\", \\\"this\\\", \\\"that\\\", and \\\"what\\\", often appearing in comparative, descriptive, or explanatory contexts. Many activations cluster around compound or multi-syllabic words where partial segments (e.g., \\\"mis\\\", \\\"stere\\\", \\\"endos\\\", \\\"cination\\\") are individually significant, suggesting that morphological subunits of complex words are important for semantic interpretation. Additionally, common phrases like \\\"according to\\\", \\\"contrary to\\\", \\\"in the\\\", and \\\"this sort of\\\" frequently appear, indicating that functional and idiomatic expressions are key to contextual understanding.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2060",
      "text": "The highlighted fragments are the core semantic units—roots, suffixes, or whole words—that carry the main meaning of a phrase or word, often appearing in idioms or common collocations, and are the most informative for the model’s activation.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1237",
        "exp_413"
      ],
      "cosine_similarity": 0.8647504856728768,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1237",
        "exp_2060"
      ],
      "cosine_similarity": 0.8425269177909283,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_413",
        "exp_2060"
      ],
      "cosine_similarity": 0.8969764430767608,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5333333333333333,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.3952
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.54,
      "score_detection": 0.46,
      "score_simulation": 0.09268371869372652,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.4,
      "score_simulation": 0.06586591605077993,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.3952
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.51,
      "score_detection": 0.43,
      "score_simulation": -0.005952180716671969,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.62,
      "score_detection": 0.56,
      "score_simulation": 0.17472373484672105,
      "score_embedding": 0.3952
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.25,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}