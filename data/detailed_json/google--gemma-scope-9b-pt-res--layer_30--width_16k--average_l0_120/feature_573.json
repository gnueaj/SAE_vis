{
  "feature_id": 573,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1297",
      "text": "Words related to family relationships, possession, or connection, often used to describe kinship or association.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_473",
      "text": "The word \\\"and\\\" frequently appears in lists or comparisons, often connecting nouns or noun phrases, particularly in contexts involving groups, relationships, or categories such as people, family members, or entities. It is commonly used to link items in a series, especially when describing social groups, familial relationships, or inclusive categories.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2120",
      "text": "The highlighted tokens are family‑related terms and modifiers that signal relationships or family units, often appearing with words like “run,” “member,” “of,” or “and.”",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1297",
        "exp_473"
      ],
      "cosine_similarity": 0.8662880410852047,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1297",
        "exp_2120"
      ],
      "cosine_similarity": 0.920665428774077,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_473",
        "exp_2120"
      ],
      "cosine_similarity": 0.8679581048508875,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.55,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.426875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.69,
      "score_detection": 0.64,
      "score_simulation": null,
      "score_embedding": 0.5864
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.775,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.525
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.525
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.75,
      "score_detection": 0.6857142857142857,
      "score_simulation": null,
      "score_embedding": 0.5864
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.426875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.76,
      "score_detection": 0.61,
      "score_simulation": null,
      "score_embedding": 0.5864
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.64,
      "score_detection": 0.66,
      "score_simulation": null,
      "score_embedding": 0.426875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.525
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}