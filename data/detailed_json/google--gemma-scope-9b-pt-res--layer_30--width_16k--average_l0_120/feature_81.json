{
  "feature_id": 81,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_893",
      "text": "Prefixes or suffixes of words, often indicating a change in meaning or grammatical function, or function words that provide context or connection between clauses.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_069",
      "text": "High activation on function words (like \\\"the\\\", \\\"of\\\", \\\"and\\\", \\\"in\\\") and suffixes (like \\\"er\\\", \\\"ing\\\", \\\"ly\\\") when they appear in contextually significant phrases, often marking grammatical structure, comparison, possession, or modification.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1717",
      "text": "The highlighted tokens are primarily function words and morphological suffixes that signal grammatical relations and form idiomatic or collocational phrases.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_893",
        "exp_069"
      ],
      "cosine_distance": 0.1077321225367569,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_893",
        "exp_1717"
      ],
      "cosine_distance": 0.09924528781081343,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_069",
        "exp_1717"
      ],
      "cosine_distance": 0.08264712394954732,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.46,
      "score_detection": 0.5,
      "score_simulation": 0.04325697266671728,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.509090909090909,
      "score_simulation": 0.04885542295732055,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.54,
      "score_detection": 0.43,
      "score_simulation": 0.006546359856823624,
      "score_embedding": 0.418
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.48,
      "score_simulation": 0.10229138809078452,
      "score_embedding": 0.366
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.475,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}