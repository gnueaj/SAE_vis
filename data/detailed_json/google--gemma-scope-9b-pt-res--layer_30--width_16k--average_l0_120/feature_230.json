{
  "feature_id": 230,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1018",
      "text": "Prepositions and articles often precede nouns, while adjectives and adverbs often precede or follow the nouns or verbs they modify, and mathematical or scientific terms often have specific symbols or formatting.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_194",
      "text": "The token \\\"the\\\" frequently appears in contexts involving mathematical or technical descriptions, often preceding nouns that denote abstract quantities, sets, or properties, and is commonly associated with formal or quantitative expressions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1841",
      "text": "The highlighted tokens are the core words of quantitative or relational phrases—common nouns, adjectives, or function words that form key expressions such as “number of”, “total number”, “time”, “value”, “function difference of”, “product of”, “sum of”, “amount of”, “percentage”, “range”, “log”, “energy”, “affine”, “radius”, “ratio”, “speed”. These phrases convey measurements, relationships, or mathematical operations, and the model activates them as important.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1018",
        "exp_194"
      ],
      "cosine_similarity": 0.8571768638657382,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1018",
        "exp_1841"
      ],
      "cosine_similarity": 0.8254339106806714,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_194",
        "exp_1841"
      ],
      "cosine_similarity": 0.8642577879894249,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5428571428571428,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.518125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.54,
      "score_detection": 0.59,
      "score_simulation": null,
      "score_embedding": 0.374
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7666666666666667,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.4325
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.4325
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.64,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.374
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.575,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.518125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.62,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.374
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.68,
      "score_detection": 0.52,
      "score_simulation": null,
      "score_embedding": 0.518125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.4325
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}