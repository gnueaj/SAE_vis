{
  "feature_id": 36,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_855",
      "text": "A variety of medical and injury-related terms, often describing conditions, procedures, or consequences, sometimes in a formal or technical context.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_031",
      "text": "Commonly activated tokens include function words (e.g., \\\"the\\\", \\\"of\\\", \\\"and\\\"), suffixes forming comparative or derived words (e.g., \\\"er\\\", \\\"ion\\\", \\\"ation\\\"), and contextually significant nouns or adjectives related to medical, legal, or physical conditions, often appearing in compound terms or idiomatic expressions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1679",
      "text": "The highlighted fragments are mostly subword pieces that belong to idiomatic or domain‑specific multi‑word expressions, often with a leading space, and include common suffixes (e.g., “‑er” for comparatives) or medical terminology, indicating that the model is detecting phrase‑level or morphological patterns rather than isolated words.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_855",
        "exp_031"
      ],
      "cosine_similarity": 0.8698511045785353,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_855",
        "exp_1679"
      ],
      "cosine_similarity": 0.8474691447350262,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_031",
        "exp_1679"
      ],
      "cosine_similarity": 0.8984164756769044,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45714285714285713,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.835
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.64,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": 0.6892
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.75,
      "score_detection": 0.6571428571428571,
      "score_simulation": null,
      "score_embedding": 0.801875
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.801875
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.6892
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.835
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.73,
      "score_detection": 0.62,
      "score_simulation": null,
      "score_embedding": 0.6892
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.77,
      "score_detection": 0.57,
      "score_simulation": null,
      "score_embedding": 0.835
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.725,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": 0.801875
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.857422205021641,
      "z_score_fuzz": -0.7219720723019047,
      "z_score_detection": -0.4249030541571479
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 1.0502927312758117,
      "z_score_fuzz": 0.11907576779547215,
      "z_score_detection": -0.05046587528065854
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.6740465958458481,
      "z_score_fuzz": 0.3019832592341254,
      "z_score_detection": 0.6566186187584249
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.23684902618752943
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.37296754126354176
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.8775494912794661
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}