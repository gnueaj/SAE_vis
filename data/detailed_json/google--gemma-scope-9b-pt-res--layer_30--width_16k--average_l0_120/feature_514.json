{
  "feature_id": 514,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1244",
      "text": "Prepositions and articles often precede nouns, while adjectives often precede nouns they describe, and sometimes words that indicate a relationship or possession are used.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_420",
      "text": "Commonly activated tokens include function words like \\\"of\\\", \\\"to\\\", \\\"for\\\", \\\"and\\\", \\\"the\\\", and \\\"s\\\", as well as adjectives and nouns that describe abstract or concrete attributes, often in compound or descriptive phrases, particularly in academic, cultural, or descriptive contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2067",
      "text": "The highlighted words are the core lexical elements of phrases—usually nouns, adjectives, or prepositions—that carry the main semantic content or form idiomatic collocations.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1244",
        "exp_420"
      ],
      "cosine_distance": 0.15313439866853884,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1244",
        "exp_2067"
      ],
      "cosine_distance": 0.1508634780644138,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_420",
        "exp_2067"
      ],
      "cosine_distance": 0.1421418456725756,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.375,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.49,
      "score_simulation": 0.07620292636615657,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5833333333333334,
      "score_detection": 0.48,
      "score_simulation": 0.17369628866701597,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.56,
      "score_detection": 0.45,
      "score_simulation": 0.047552926152189826,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.44,
      "score_simulation": 0.024251536805413074,
      "score_embedding": 0.4572
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.475,
      "score_detection": 0.2,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}