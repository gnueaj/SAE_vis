{
  "feature_id": 133,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_934",
      "text": "Proper nouns, technical terms, and specific words that provide context or clarity in a sentence, often denoting names, locations, concepts, or objects.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_110",
      "text": "Repeated or recurring tokens, often part of compound words, proper nouns, or technical terms, that appear in contexts involving specific domains such as programming, scientific notation, or named entities, with activation values indicating their importance in context.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1758",
      "text": "The highlighted words are the core lexical units that together form a semantically salient phrase or unit—often a noun phrase, a comparative adjective (including the “‑er” suffix), or a key term in a technical or cultural context. They carry the main meaning of the surrounding phrase.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_934",
        "exp_110"
      ],
      "cosine_distance": 0.13006498779795417,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_934",
        "exp_1758"
      ],
      "cosine_distance": 0.14160145640816157,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_110",
        "exp_1758"
      ],
      "cosine_distance": 0.15460967742303222,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.375,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.69,
      "score_detection": 0.65,
      "score_simulation": 0.10218427035759574,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.775,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.4666666666666667,
      "score_simulation": 0.14762480581450085,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.68,
      "score_detection": 0.71,
      "score_simulation": 0.10076954887437888,
      "score_embedding": 0.4164
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.29,
      "score_detection": 0.25,
      "score_simulation": -0.03463710651403617,
      "score_embedding": 0.34959999999999997
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.775,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}