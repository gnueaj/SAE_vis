{
  "feature_id": 451,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1194",
      "text": "Abbreviations, names of diseases, medical conditions, treatments, and organizations, often in the context of scientific or medical research.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_370",
      "text": "Partial or truncated proper nouns, acronyms, or medical terms often appear in scientific text with internal tokenization, where the model activates on fragments of compound terms, especially at the boundaries of abbreviations, gene names, disease names, or journal references.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2017",
      "text": "The highlighted tokens are domain‑specific named entities—medical terms, drug names, disease names, and abbreviations—often multi‑word phrases or capitalized abbreviations that the model treats as key features.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1194",
        "exp_370"
      ],
      "cosine_similarity": 0.8923248898634766,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1194",
        "exp_2017"
      ],
      "cosine_similarity": 0.8801988839213118,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_370",
        "exp_2017"
      ],
      "cosine_similarity": 0.8886605279802257,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.4857142857142857,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.61,
      "score_detection": 0.49,
      "score_simulation": 0.06885819900459088,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5875,
      "score_detection": 0.4533333333333333,
      "score_simulation": 0.06843004391174881,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.69,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.43239999999999995
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.56,
      "score_simulation": 0.07136332999452023,
      "score_embedding": 0.322
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.575,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}