{
  "feature_id": 984,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1633",
      "text": "Proper nouns, names of people, places, organizations, and specific titles, often written in their native languages or with special characters.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_809",
      "text": "Partial or fragmented tokens, often representing parts of names, places, or words, particularly in multilingual or compound contexts, with high activation on suffixes, prefixes, or phonetic segments that are part of larger lexical units.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2456",
      "text": "The highlighted segments are consistently multi‑token units that together convey a single semantic entity—idiomatic expressions, proper‑noun phrases, or noun phrases—often spanning multiple languages. Each token within the unit receives a high activation, indicating that the model treats the entire sequence as a cohesive concept rather than isolated words.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1633",
        "exp_809"
      ],
      "cosine_similarity": 0.8413824594800209,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1633",
        "exp_2456"
      ],
      "cosine_similarity": 0.8234649797586486,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_809",
        "exp_2456"
      ],
      "cosine_similarity": 0.8729109862821874,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.55,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.6925
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.67,
      "score_detection": 0.54,
      "score_simulation": null,
      "score_embedding": 0.6628000000000001
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.6818750000000001
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.6818750000000001
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.6628000000000001
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.6925
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.52,
      "score_simulation": null,
      "score_embedding": 0.6628000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.6925
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.6818750000000001
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.0685610938503056,
      "z_score_fuzz": -0.3267612425862445,
      "z_score_detection": -0.41320189231725774
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.9041458306798592,
      "z_score_fuzz": -0.39535155187573934,
      "z_score_detection": -0.2142821410391231
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.009742502227882,
      "z_score_fuzz": -0.61255419795914,
      "z_score_detection": -0.47170770151670893
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.10953265298226779
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.09817071258833225
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.024839799082655618
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}