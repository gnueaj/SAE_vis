{
  "feature_id": 892,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1557",
      "text": "Mathematical notation and symbols, often used in equations and formulas, including LaTeX syntax and formatting.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_733",
      "text": "Mathematical notation and symbolic expressions in LaTeX, particularly involving special characters, delimiters, and structured formatting, are consistently highlighted.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2380",
      "text": "The highlighted tokens are typically key words that anchor idiomatic or technical phrases—common function words such as “the” or “in,” domain terms like “polynomial” or “mathbb,” and suffixes such as “er” that signal comparative forms.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1557",
        "exp_733"
      ],
      "cosine_similarity": 0.9085263761260752,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1557",
        "exp_2380"
      ],
      "cosine_similarity": 0.8251942629195467,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_733",
        "exp_2380"
      ],
      "cosine_similarity": 0.8513207163100495,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.925,
      "score_detection": 0.9,
      "score_simulation": null,
      "score_embedding": 0.931875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.69,
      "score_detection": 0.93,
      "score_simulation": null,
      "score_embedding": 0.9304000000000001
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.55,
      "score_detection": 0.45714285714285713,
      "score_simulation": null,
      "score_embedding": 0.83375
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.83375
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.9,
      "score_detection": 0.9,
      "score_simulation": null,
      "score_embedding": 0.9304000000000001
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": 0.931875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.84,
      "score_detection": 0.91,
      "score_simulation": null,
      "score_embedding": 0.9304000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.8,
      "score_detection": 0.925,
      "score_simulation": null,
      "score_embedding": 0.931875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.75,
      "score_simulation": null,
      "score_embedding": 0.83375
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}