{
  "feature_id": 267,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1049",
      "text": "Special characters and symbols used in programming languages, such as operators, template and generic type declarations, and namespace separators.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_225",
      "text": "The token sequences often represent syntactic or structural elements in programming languages, such as keywords, type names, operators, or identifiers, frequently appearing in code contexts with specific formatting or naming conventions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1872",
      "text": "The highlighted tokens are code identifiers, keywords, or syntax symbols that mark significant parts of programming constructs.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1049",
        "exp_225"
      ],
      "cosine_similarity": 0.8805638108612931,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1049",
        "exp_1872"
      ],
      "cosine_similarity": 0.8742962048929005,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_225",
        "exp_1872"
      ],
      "cosine_similarity": 0.9253936238633971,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.62125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.42,
      "score_detection": 0.42,
      "score_simulation": null,
      "score_embedding": 0.6764
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.5975
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.5975
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.36666666666666664,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.6764
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.62125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4,
      "score_detection": 0.46,
      "score_simulation": null,
      "score_embedding": 0.6764
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.61,
      "score_detection": 0.43,
      "score_simulation": null,
      "score_embedding": 0.62125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.5975
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}