{
  "feature_id": 379,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1140",
      "text": "Punctuation and special characters, often used to denote code, mathematical expressions, or formatting, and sometimes preceding or following specific words or phrases.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_316",
      "text": "The presence of tokens that form compound words, abbreviations, or technical terms, often separated by underscores, hyphens, or special characters, with high activation values indicating structural or semantic significance in context.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1963",
      "text": "The highlighted tokens consistently form coherent units—phrases, idioms, or code fragments—that carry the core meaning or function of the surrounding text. These units are usually made up of nouns, verbs, adjectives, or structural punctuation, and the importance scores reflect how central each element is to that unit.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1140",
        "exp_316"
      ],
      "cosine_distance": 0.14571697347780832,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1140",
        "exp_1963"
      ],
      "cosine_distance": 0.17632960185338087,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_316",
        "exp_1963"
      ],
      "cosine_distance": 0.12399207679704305,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.425,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.42,
      "score_detection": 0.44,
      "score_simulation": -0.011525343577752855,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.375,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.43333333333333335,
      "score_detection": 0.45714285714285713,
      "score_simulation": -0.04521005879995133,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.44,
      "score_detection": 0.41,
      "score_simulation": -0.03740284350499082,
      "score_embedding": 0.5816000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.54,
      "score_detection": 0.44,
      "score_simulation": -0.0830563259420159,
      "score_embedding": 0.66
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.2,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}