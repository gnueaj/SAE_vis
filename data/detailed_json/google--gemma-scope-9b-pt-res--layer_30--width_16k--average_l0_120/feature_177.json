{
  "feature_id": 177,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_973",
      "text": "Punctuation marks and short function words that connect clauses or phrases, often used to indicate relationships between ideas or to provide additional information.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_149",
      "text": "Common linguistic patterns involving punctuation, abbreviations, compound terms, and contextual phrases that frequently appear in technical, legal, or structured text, often signaling specific syntactic or semantic roles such as emphasis, formatting, or reference.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1797",
      "text": "The highlighted tokens are the core lexical items of idiomatic or collocational phrases, often the main word in a multiâ€‘word expression or a morphological marker that carries the key semantic content.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_973",
        "exp_149"
      ],
      "cosine_similarity": 0.8614500822530592,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_973",
        "exp_1797"
      ],
      "cosine_similarity": 0.8261104719063559,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_149",
        "exp_1797"
      ],
      "cosine_similarity": 0.8615844496581408,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.25,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.240625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.51,
      "score_detection": 0.54,
      "score_simulation": null,
      "score_embedding": 0.33759999999999996
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": 0.26125000000000004
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.26125000000000004
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6857142857142857,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": 0.33759999999999996
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.240625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.58,
      "score_detection": 0.68,
      "score_simulation": null,
      "score_embedding": 0.33759999999999996
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.33,
      "score_detection": 0.32,
      "score_simulation": null,
      "score_embedding": 0.240625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": 0.26125000000000004
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.4329590086798503,
      "z_score_fuzz": -2.201563029832436,
      "z_score_detection": -1.5950192381461754
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.8961182630248311,
      "z_score_fuzz": -0.3822867310586923,
      "z_score_detection": 0.20194490155125958
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.318781742589262,
      "z_score_fuzz": -0.5553956068845605,
      "z_score_detection": 0.8739259100706722
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.7431804255528205
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.3588200308440879
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.33341714646771675
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}