{
  "feature_id": 213,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1003",
      "text": "Tokens that are part of a larger code or programming syntax, often including symbols, operators, and keywords, and sometimes numerical values or mathematical expressions.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_179",
      "text": "The token sequences often represent grammatical or syntactic elements, numerical values, or identifiers in code, mathematical expressions, or structured text, with high activation on words that serve as connectors, modifiers, or markers of structure, such as prepositions, conjunctions, punctuation, or parts of compound terms.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1827",
      "text": "The highlighted tokens are the core lexical or structural elements that carry the main semantic or syntactic weight of a phrase or code fragment, often the first token of a multiâ€‘token phrase or a key punctuation or symbol that signals a concept or operation.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1003",
        "exp_179"
      ],
      "cosine_distance": 0.09200210881379933,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1003",
        "exp_1827"
      ],
      "cosine_distance": 0.1172717739168323,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_179",
        "exp_1827"
      ],
      "cosine_distance": 0.10663220053657341,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.78,
      "score_detection": 0.82,
      "score_simulation": 0.02621629623749137,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6666666666666666,
      "score_detection": 0.75,
      "score_simulation": -0.01961508590019147,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.76,
      "score_detection": 0.81,
      "score_simulation": 0.03409398964003287,
      "score_embedding": 0.576
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.69,
      "score_detection": 0.71,
      "score_simulation": -0.05179277754490285,
      "score_embedding": 0.5831999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}