{
  "feature_id": 770,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1463",
      "text": "XML/HTML tags and mathematical symbols, often denoting variables, parameters, or formatting styles.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_639",
      "text": "Tokens enclosed in delimiters often represent structural or semantic placeholders in code, markup, or templating languages, such as tags, attributes, identifiers, or variables, with higher activation values indicating more critical syntactic or contextual roles.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2286",
      "text": "The highlighted tokens are typically short, singleâ€‘word identifiers or keywords that function as tag names, attribute names, or variable names across a range of code and markup contexts (HTML, XML, LaTeX, programming languages). They recur in similar syntactic positions, suggesting a pattern of concise identifiers used as structural or placeholder elements.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1463",
        "exp_639"
      ],
      "cosine_similarity": 0.8668754991151388,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1463",
        "exp_2286"
      ],
      "cosine_similarity": 0.8664547474597382,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_639",
        "exp_2286"
      ],
      "cosine_similarity": 0.9075533198040027,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.75,
      "score_detection": 0.925,
      "score_simulation": null,
      "score_embedding": 0.6656250000000001
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.53,
      "score_detection": 0.64,
      "score_simulation": null,
      "score_embedding": 0.6248
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.455625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.455625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.84,
      "score_detection": 0.925,
      "score_simulation": null,
      "score_embedding": 0.6248
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.6656250000000001
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.74,
      "score_detection": 0.71,
      "score_simulation": null,
      "score_embedding": 0.6248
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.725,
      "score_detection": 0.75,
      "score_simulation": null,
      "score_embedding": 0.6656250000000001
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.455625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.9197846562171159,
      "z_score_fuzz": 0.13050748601038745,
      "z_score_detection": 1.6345014296635407
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.6937828677008362,
      "z_score_fuzz": 0.3820052867385363,
      "z_score_detection": 1.400478192865735
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.24274750761432654,
      "z_score_fuzz": 0.24482466815954507,
      "z_score_detection": 0.05484458127835328
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.894931190630348
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.8254221157683692
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.018973913941190603
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}