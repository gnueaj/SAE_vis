{
  "feature_id": 477,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1216",
      "text": "Punctuation marks, often periods, that end sentences or clauses.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_392",
      "text": "The period (\\\".\\\") is frequently activated in contexts involving punctuation boundaries, especially at the end of sentences, clauses, or after closing brackets, quotation marks, or mathematical expressions, often signaling syntactic or structural completion.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2039",
      "text": "The highlighted tokens are almost always punctuation marks or very short words that appear at the ends of phrases or sentences, suggesting the model is picking up on boundary and structural cues rather than substantive lexical content.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1216",
        "exp_392"
      ],
      "cosine_similarity": 0.9165511611947827,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1216",
        "exp_2039"
      ],
      "cosine_similarity": 0.856089495486108,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_392",
        "exp_2039"
      ],
      "cosine_similarity": 0.8618454382860319,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.82,
      "score_detection": 0.49,
      "score_simulation": 0.5278592784626903,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7764705882352941,
      "score_detection": 0.6133333333333333,
      "score_simulation": 0.6479567018667789,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.75,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.93,
      "score_detection": 0.56,
      "score_simulation": 0.2029386048966852,
      "score_embedding": 0.44919999999999993
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.87,
      "score_detection": 0.5,
      "score_simulation": 0.35980578065856095,
      "score_embedding": 0.44079999999999997
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}