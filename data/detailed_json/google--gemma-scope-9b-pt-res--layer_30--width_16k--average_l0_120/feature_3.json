{
  "feature_id": 3,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_828",
      "text": "Various tokens including numbers, mathematical symbols, and common function words, often used in formal or technical writing, such as academic or scientific texts, and sometimes in news articles or other informative content.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_004",
      "text": "The token sequences often represent grammatical or syntactic elements, numerical values, or parts of compound terms, with activations frequently occurring at morphological suffixes, punctuation, or boundary markers in structured or technical text.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1652",
      "text": "The pattern? The tokens are often words or fragments that appear in the text.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_828",
        "exp_004"
      ],
      "cosine_similarity": 0.9008368454955525,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_828",
        "exp_1652"
      ],
      "cosine_similarity": 0.8561256204942452,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_004",
        "exp_1652"
      ],
      "cosine_similarity": 0.872300530252069,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.525,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.535625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.56,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": 0.518
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.5142857142857142,
      "score_simulation": null,
      "score_embedding": 0.43437499999999996
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.43437499999999996
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.518
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.535625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.56,
      "score_detection": 0.46,
      "score_simulation": null,
      "score_embedding": 0.518
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.51,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.535625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.35,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.43437499999999996
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.2001218881309847,
      "z_score_fuzz": -0.8754837169022026,
      "z_score_detection": -0.41320189231725774
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.10255222438084581,
      "z_score_fuzz": -0.8526202804723713,
      "z_score_detection": -0.24938562655879395
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.36038469085917507,
      "z_score_fuzz": -1.4127744730032454,
      "z_score_detection": -0.49678161974504553
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.3628545736961586
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.33315122755010645
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.7566469278691553
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}