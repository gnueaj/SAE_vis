{
  "feature_id": 603,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1320",
      "text": "Mathematical and scientific notation, including symbols, equations, and formatting, often used in academic or technical contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_496",
      "text": "The token sequences often involve mathematical expressions, symbolic notation, or technical terminology, with activations frequently occurring at punctuation, operators, subscripts, superscripts, or specific structural markers in equations and formal notation.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2143",
      "text": "The highlighted tokens consistently represent the essential lexical or symbolic components that form idiomatic phrases, comparative endings, key nouns, or functional symbols, serving as the core units that drive the meaning or operation of the surrounding text.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1320",
        "exp_496"
      ],
      "cosine_similarity": 0.9016349002738365,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1320",
        "exp_2143"
      ],
      "cosine_similarity": 0.8146955918817524,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_496",
        "exp_2143"
      ],
      "cosine_similarity": 0.8467546146652882,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.42857142857142855,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.46062500000000006
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.52,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": 0.596
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5333333333333333,
      "score_detection": 0.5142857142857142,
      "score_simulation": null,
      "score_embedding": 0.4325
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.4325
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.596
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.46062500000000006
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.56,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": 0.596
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.56,
      "score_detection": 0.62,
      "score_simulation": null,
      "score_embedding": 0.46062500000000006
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.4325
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.21506817038024426,
      "z_score_fuzz": -1.0387939771152854,
      "z_score_detection": -0.1323740081598908
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.5343498852325239,
      "z_score_fuzz": -0.8297568440425391,
      "z_score_detection": -0.4834088633565995
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.3707644423219556,
      "z_score_fuzz": -0.7078185164167714,
      "z_score_detection": -0.3212641921466916
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.46207871855180677
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.25960527405553824
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.4666157169618062
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}