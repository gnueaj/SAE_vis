{
  "feature_id": 229,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1017",
      "text": "Prepositions, articles, and conjunctions, often used in phrases or sentences to connect ideas or show relationships between words, and sometimes nouns that represent objects or concepts.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_193",
      "text": "High activation on function words (e.g., \\\"to\\\", \\\"for\\\", \\\"in\\\", \\\"is\\\", \\\"a\\\", \\\"the\\\") and common morphological suffixes (e.g., \\\"er\\\", \\\"ing\\\", \\\"ly\\\") when they appear in syntactically or semantically pivotal positions, particularly in technical, formal, or structured text. These tokens often serve grammatical roles or form part of compound terms, especially in scientific, legal, or programming contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1840",
      "text": "The highlighted tokens are the core lexical units—whole words or meaningful word fragments—that together compose a phrase or a key part of a word, often including surrounding spaces or punctuation to mark word boundaries.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1017",
        "exp_193"
      ],
      "cosine_similarity": 0.839171100292598,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1017",
        "exp_1840"
      ],
      "cosine_similarity": 0.8353750223477738,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_193",
        "exp_1840"
      ],
      "cosine_similarity": 0.8745360496420969,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.44,
      "score_detection": 0.36666666666666664,
      "score_simulation": null,
      "score_embedding": 0.5004000000000001
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.52,
      "score_detection": 0.44,
      "score_simulation": 0.03933850337491828,
      "score_embedding": 0.4396
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5666666666666667,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.45,
      "score_simulation": 0.011245663472310164,
      "score_embedding": 0.4396
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.375,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.5004000000000001
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4,
      "score_detection": 0.44,
      "score_simulation": 0.013489354463800634,
      "score_embedding": 0.4396
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.45,
      "score_detection": 0.36,
      "score_simulation": -0.035793329776687505,
      "score_embedding": 0.5004000000000001
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}