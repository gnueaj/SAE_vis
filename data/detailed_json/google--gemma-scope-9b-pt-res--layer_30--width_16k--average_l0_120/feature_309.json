{
  "feature_id": 309,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1083",
      "text": "Prepositions, conjunctions, and adverbs often function as important tokens, particularly when used in phrases or idiomatic expressions, and sometimes when used to connect clauses or phrases.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_259",
      "text": "The token \\\"of\\\" frequently appears in prepositional phrases indicating possession, quantity, or duration, often preceding nouns that denote abstract or measurable concepts. Other common tokens include \\\"and\\\", \\\"or\\\", \\\"more\\\", \\\"much\\\", \\\"less\\\", and comparative suffixes like \\\"-er\\\", which appear in contexts involving comparison, addition, or degree. Additionally, hyphenated compounds and adjectives describing quality or intensity are often activated, suggesting attention to linguistic structures expressing gradation, inclusion, or modification.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1906",
      "text": "The highlighted tokens are the core function words or suffixes that assemble into familiar multi‑word expressions—idioms, comparative endings, or container nouns—providing the connective or descriptive glue that gives the phrase its meaning.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1083",
        "exp_259"
      ],
      "cosine_similarity": 0.8765998294411638,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1083",
        "exp_1906"
      ],
      "cosine_similarity": 0.8851337614588053,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_259",
        "exp_1906"
      ],
      "cosine_similarity": 0.8739404410420798,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.4728
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.47,
      "score_detection": 0.49,
      "score_simulation": 0.13241745743319527,
      "score_embedding": 0.426
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6285714285714286,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.45714285714285713,
      "score_simulation": 0.16078030063758092,
      "score_embedding": 0.426
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.4728
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.63,
      "score_detection": 0.51,
      "score_simulation": 0.10045948019205257,
      "score_embedding": 0.426
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.74,
      "score_detection": 0.56,
      "score_simulation": 0.43344557148831836,
      "score_embedding": 0.4728
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}