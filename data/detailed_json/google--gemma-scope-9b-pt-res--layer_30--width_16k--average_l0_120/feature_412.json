{
  "feature_id": 412,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1163",
      "text": "Code snippets from various programming languages, often containing method calls, variable assignments, and mathematical operations, with a focus on graphical and visual elements.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_339",
      "text": "The presence of identifiers, method calls, and syntax elements in programming code, particularly those involving object-oriented or functional constructs, with high activation on tokens that denote structure, access, or operations.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1986",
      "text": "The highlighted tokens are code elements that form syntactic units in programming languages, such as identifiers, operators, punctuation, and literals, often grouped into contiguous sequences that represent function calls, method invocations, or expressions.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1163",
        "exp_339"
      ],
      "cosine_similarity": 0.8959456384704553,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1163",
        "exp_1986"
      ],
      "cosine_similarity": 0.8907695290167092,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_339",
        "exp_1986"
      ],
      "cosine_similarity": 0.9289612338740182,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.495625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.48,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.542
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5666666666666667,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.5349999999999999
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.5349999999999999
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.425,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.542
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.495625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.47,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.542
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.43,
      "score_detection": 0.44,
      "score_simulation": null,
      "score_embedding": 0.495625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.375,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.5349999999999999
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.02131280974167108,
      "z_score_fuzz": -1.3441841637137506,
      "z_score_detection": -0.6706274527948439
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.23541304310443928,
      "z_score_fuzz": -1.2984572908540875,
      "z_score_detection": -0.6940297764746247
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.1966619709767239,
      "z_score_fuzz": -1.3175101545456138,
      "z_score_detection": -0.8227425567134172
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.6787081420834218
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.585691341408091
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.6478635800941024
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}