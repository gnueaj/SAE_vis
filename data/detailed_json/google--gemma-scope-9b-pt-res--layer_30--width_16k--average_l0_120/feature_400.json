{
  "feature_id": 400,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1155",
      "text": "Specialized terms and identifiers in programming languages, often denoting variables, functions, or constants, and sometimes referencing external libraries or frameworks.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_331",
      "text": "Common patterns in code include identifiers with underscores separating words, repeated use of capitalized terms like SHADOW, OpenGL, UnityEngine, and fragment, often in context of graphics programming, shader code, or software development, with frequent activation of tokens related to structure, naming conventions, and technical components.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1978",
      "text": "The highlighted tokens are consistently parts of programming identifiers—often uppercase, underscore‑separated names or key graphics/shader terms such as “SHADOW”, “SOFT”, “fragment”, “texture”, “buffer”, “vertex”, “OpenGL”, “GLSL”, “UnityEngine”, etc. They appear within code snippets and represent variables, constants, or function names tied to rendering or shader logic.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1155",
        "exp_331"
      ],
      "cosine_similarity": 0.8756351887461207,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1155",
        "exp_1978"
      ],
      "cosine_similarity": 0.8901837063014045,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_331",
        "exp_1978"
      ],
      "cosine_similarity": 0.939157779987424,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.275,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.41250000000000003
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.54,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.6135999999999999
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.38625000000000004
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.225,
      "score_simulation": null,
      "score_embedding": 0.38625000000000004
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": 0.6135999999999999
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.375,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": 0.41250000000000003
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.51,
      "score_detection": 0.52,
      "score_simulation": null,
      "score_embedding": 0.6135999999999999
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.47,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.41250000000000003
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.375,
      "score_detection": 0.25,
      "score_simulation": null,
      "score_embedding": 0.38625000000000004
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.48148179125828316,
      "z_score_fuzz": -1.881474919814793,
      "z_score_detection": -1.4195018105478214
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.6317811522964921,
      "z_score_fuzz": -0.8983471533320341,
      "z_score_detection": -0.7993402330336369
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.6267983117372133,
      "z_score_fuzz": -1.1841401087049293,
      "z_score_detection": -2.1683761683007994
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.260819507206966
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.3553020780230596
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -1.3264381962476472
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}