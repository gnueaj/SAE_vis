{
  "feature_id": 184,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_980",
      "text": "Function words and common prepositions, conjunctions, and articles, often used to connect clauses or phrases, or to indicate relationships between entities.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_156",
      "text": "Common function words and short phrases that serve grammatical or contextual衔接, such as prepositions, conjunctions, comparative markers, and discourse markers, often appear in high activation states when they connect ideas, modify meaning, or structure sentences.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1804",
      "text": "The highlighted tokens are predominantly short, high‑frequency function words—prepositions, conjunctions, determiners—and a few key content words that signal grammatical relations or core semantic content; they serve as the connective tissue that links clauses and indicates relationships within the sentence.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_980",
        "exp_156"
      ],
      "cosine_distance": 0.08019784442785638,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_980",
        "exp_1804"
      ],
      "cosine_distance": 0.07749104102711069,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_156",
        "exp_1804"
      ],
      "cosine_distance": 0.077998608370624,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.51,
      "score_detection": 0.53,
      "score_simulation": -0.00024204615742556346,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.42,
      "score_detection": 0.5846153846153846,
      "score_simulation": -0.009367018669578214,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.59,
      "score_detection": 0.61,
      "score_simulation": -0.005338102622654061,
      "score_embedding": 0.732
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.57,
      "score_detection": 0.68,
      "score_simulation": 0.17565426953821628,
      "score_embedding": 0.816
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}