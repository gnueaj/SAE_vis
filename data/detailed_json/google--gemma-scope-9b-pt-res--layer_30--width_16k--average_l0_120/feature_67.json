{
  "feature_id": 67,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_881",
      "text": "Negation and degree adverbs, often used in informal or conversational contexts, typically in the form of auxiliary verbs or adverbs like \\\"not\\\", \\\"yet\\\", \\\"quite\\\", and \\\"t\\\", which are used to express contrast, uncertainty, or degree.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_057",
      "text": "The use of negation markers (\\\"not\\\", \\\"n't\\\") and temporal qualifiers (\\\"yet\\\", \\\"quite\\\") in contexts indicating incompleteness, absence, or unfulfilled expectations.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1705",
      "text": "The highlighted tokens are short, high‑frequency function words that form common collocations—especially negation and emphasis such as “not yet,” “quite,” “you,” “enough,” and the fragment “t.” They serve as grammatical connectors or particles rather than content words, and they recur across the examples.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_881",
        "exp_057"
      ],
      "cosine_similarity": 0.9124710504626796,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_881",
        "exp_1705"
      ],
      "cosine_similarity": 0.8594688377009846,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_057",
        "exp_1705"
      ],
      "cosine_similarity": 0.8500788360960315,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.85,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": 0.704375
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.9,
      "score_detection": 0.87,
      "score_simulation": null,
      "score_embedding": 0.44880000000000003
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.15625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.825,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.15625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": 0.44880000000000003
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": 0.704375
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.88,
      "score_detection": 0.87,
      "score_simulation": null,
      "score_embedding": 0.44880000000000003
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.87,
      "score_detection": 0.89,
      "score_simulation": null,
      "score_embedding": 0.704375
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.825,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": 0.15625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.1342995197812502,
      "z_score_fuzz": 1.3194061803616308,
      "z_score_detection": 2.0206397703799204
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.2805298029388482,
      "z_score_fuzz": 1.2851110257168834,
      "z_score_detection": 2.079145579579371
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.900047824504983,
      "z_score_fuzz": 0.7592519878307565,
      "z_score_detection": 1.1079491468684792
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 1.4914484901742668
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 1.0279089341191354
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.01094889660191578
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}