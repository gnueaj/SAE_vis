{
  "feature_id": 32,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_852",
      "text": "Tokens that are part of a code, mathematical expression, or technical term, often including special characters and symbols.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_028",
      "text": "Fragments of text containing structural or formatting markers, often surrounded by delimiters, with high activation on punctuation, symbols, or repeated patterns like \\\"<<\\\" and \\\">>\\\", indicating the presence of code, markup, or formatting syntax.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1676",
      "text": "The highlighted tokens consistently form coherent semantic units—such as noun phrases, verb phrases, or key technical terms—that carry the main meaning of the surrounding text, regardless of whether the context is natural language, code, or markup.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_852",
        "exp_028"
      ],
      "cosine_similarity": 0.8859013881833393,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_852",
        "exp_1676"
      ],
      "cosine_similarity": 0.8649454942133579,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_028",
        "exp_1676"
      ],
      "cosine_similarity": 0.855686859357473,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.533125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.58,
      "score_simulation": null,
      "score_embedding": 0.536
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.525,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.435
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.325,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.435
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7428571428571429,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.536
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.533125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.57,
      "score_simulation": null,
      "score_embedding": 0.536
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.56,
      "score_detection": 0.67,
      "score_simulation": null,
      "score_embedding": 0.533125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.3,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": 0.435
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}