{
  "feature_id": 113,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_917",
      "text": "Nouns or words that represent a distinct object, concept, or idea, often in a formal or technical context, and sometimes in a specific field such as science, technology, or medicine.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_093",
      "text": "Commonly activated tokens are often suffixes or parts of compound words, particularly in technical, scientific, or formal contexts, with frequent emphasis on specific morphological endings like \\\"-er\\\", \\\"-ing\\\", \\\"-ion\\\", \\\"-ity\\\", and \\\"-al\\\", as well as standalone terms in proper nouns, technical jargon, or domain-specific terminology.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1741",
      "text": "The highlighted tokens are the words that the model’s activation signals as most relevant to the surrounding context, typically nouns, adjectives, or key terms that carry semantic weight, including idiomatic phrases, comparative suffixes, or domain‑specific entities.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_917",
        "exp_093"
      ],
      "cosine_distance": 0.12207707245946064,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_917",
        "exp_1741"
      ],
      "cosine_distance": 0.14291245477796943,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_093",
        "exp_1741"
      ],
      "cosine_distance": 0.10128776742352419,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.48,
      "score_detection": 0.45714285714285713,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.49,
      "score_detection": 0.39,
      "score_simulation": 0.12737131122339998,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.2,
      "score_simulation": 0.1316620940969535,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.41,
      "score_detection": 0.28,
      "score_simulation": 0.0918712649236733,
      "score_embedding": 0.6088
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.58,
      "score_detection": 0.27,
      "score_simulation": -0.00881984958827679,
      "score_embedding": 0.5184
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.575,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}