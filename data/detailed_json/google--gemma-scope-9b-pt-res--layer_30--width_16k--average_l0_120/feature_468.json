{
  "feature_id": 468,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1210",
      "text": "Special characters, punctuation, and short words or abbreviations often used in technical or formal writing, such as mathematical or programming notation, and sometimes nouns or words in specific contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_386",
      "text": "The presence of delimiters or placeholder tokens (like <<>> or <<>>), often surrounding text fragments, punctuation, or symbols, indicating structural or syntactic placeholders in code, formatting, or markup, with activations primarily on single characters, symbols, or short sequences that serve as markers or separators.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2033",
      "text": "The highlighted tokens are the core content words or symbols that carry the main semantic or functional weight in each snippet—whether they are key nouns, verbs, adjectives, technical terms, numbers, or punctuation—often forming meaningful phrases or code identifiers that define the central idea or operation.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1210",
        "exp_386"
      ],
      "cosine_similarity": 0.8743023055019765,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1210",
        "exp_2033"
      ],
      "cosine_similarity": 0.8550364328680169,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_386",
        "exp_2033"
      ],
      "cosine_similarity": 0.8548418814880379,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.298125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.41,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.3268
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.37142857142857144,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.34125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.34125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.3268
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.425,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.298125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.28,
      "score_detection": 0.43,
      "score_simulation": null,
      "score_embedding": 0.3268
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.45,
      "score_detection": 0.31,
      "score_simulation": null,
      "score_embedding": 0.298125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.3,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.34125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.114646630487908,
      "z_score_fuzz": -1.2984572908540875,
      "z_score_detection": -0.7993402330336369
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.955905631450448,
      "z_score_fuzz": -1.7785894558805508,
      "z_score_detection": -0.6355239672751731
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.875912346843951,
      "z_score_fuzz": -1.9353673056851104,
      "z_score_detection": -0.6472251291150637
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.0708147181252106
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.123339684868724
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -1.1528349272147083
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}