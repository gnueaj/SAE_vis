{
  "feature_id": 134,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_935",
      "text": "Punctuation marks, often used to denote the start or end of a quotation, section, or code snippet, and sometimes used to separate items or indicate a break in thought.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_111",
      "text": "The token sequences \\\"<<\\\" and \\\">>\\\" are used to delimit content segments, often surrounding text that is contextually or structurally significant, such as quoted material, code, mathematical expressions, or metadata, with activations primarily occurring at the boundaries and within the delimiters.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1759",
      "text": "The highlighted tokens are not ordinary words but structural markers—delimiters, punctuation, whitespace, or formatting tags—that signal the boundaries or emphasis of a text segment.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_935",
        "exp_111"
      ],
      "cosine_distance": 0.13146780969229022,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_935",
        "exp_1759"
      ],
      "cosine_distance": 0.1408529293934393,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_111",
        "exp_1759"
      ],
      "cosine_distance": 0.10576522078300854,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.52,
      "score_detection": 0.48,
      "score_simulation": 0.13658813228978822,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.56,
      "score_detection": 0.425,
      "score_simulation": 0.1111871415985689,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.59,
      "score_detection": 0.37,
      "score_simulation": 0.14393776652790818,
      "score_embedding": 0.3724
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.42,
      "score_detection": 0.41,
      "score_simulation": null,
      "score_embedding": 0.258
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}