{
  "feature_id": 406,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1158",
      "text": "Tokens that are part of a larger sequence or phrase, often including punctuation, and may be related to various concepts such as programming, mathematics, or everyday language.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_334",
      "text": "Fragments of text containing technical, mathematical, or structured syntax elements, often with embedded identifiers, symbols, or formatting markers, where important tokens are typically short, context-specific, and frequently appear in sequences related to code, equations, citations, or markup.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1981",
      "text": "The highlighted fragments are contiguous character sequences that belong to larger lexical items—often the final part of a word or a component of a multi‑word expression. They can be whole words (e.g., “house”, “box”) or partial stems (e.g., “er” from “wider”, “er” from “taller”). These fragments are extracted for analysis, regardless of whether they form a complete word, a noun phrase, or a compound term.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1158",
        "exp_334"
      ],
      "cosine_similarity": 0.9027582115789061,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1158",
        "exp_1981"
      ],
      "cosine_similarity": 0.8731421771651299,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_334",
        "exp_1981"
      ],
      "cosine_similarity": 0.8705001045533428,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.325,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.8232
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.51,
      "score_detection": 0.48,
      "score_simulation": 0.040978960086996474,
      "score_embedding": 0.7476
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4666666666666667,
      "score_detection": 0.32,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.4857142857142857,
      "score_simulation": 0.0355045131545389,
      "score_embedding": 0.7476
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.8232
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.46,
      "score_detection": 0.37,
      "score_simulation": null,
      "score_embedding": 0.7476
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.58,
      "score_detection": 0.68,
      "score_simulation": 0.15701628054866465,
      "score_embedding": 0.8232
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}