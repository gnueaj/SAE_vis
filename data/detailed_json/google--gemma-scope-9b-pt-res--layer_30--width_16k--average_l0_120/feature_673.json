{
  "feature_id": 673,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1380",
      "text": "Names, labels, and identifiers of various entities, including people, places, objects, and concepts, often used to specify or reference them uniquely.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_556",
      "text": "The token \\\"name\\\" frequently appears in contexts involving identification, labeling, or referencing entities, often in proximity to possessive forms, quotation marks, or structured data fields, and is commonly associated with proper nouns, identifiers, or descriptive labels.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2203",
      "text": "The highlighted tokens are typically short, high‑frequency words that function as connectors or identifiers—most often “name” and its variants, along with prepositions and articles such as “to,” “the,” “of,” “and,” and “for.” These words usually appear in code‑style or naming contexts, forming phrases that point to a variable, attribute, or proper noun.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1380",
        "exp_556"
      ],
      "cosine_similarity": 0.8948963270722496,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1380",
        "exp_2203"
      ],
      "cosine_similarity": 0.8658344996347926,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_556",
        "exp_2203"
      ],
      "cosine_similarity": 0.8987062261568726,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.47,
      "score_detection": 0.49,
      "score_simulation": 0.0186555804189452,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.4588235294117647,
      "score_simulation": 0.02643553252646269,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.52,
      "score_detection": 0.47,
      "score_simulation": -0.0033543230984937776,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.63,
      "score_simulation": 0.1924883575718611,
      "score_embedding": 0.6591999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}