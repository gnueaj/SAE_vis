{
  "feature_id": 105,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_911",
      "text": "Special characters and symbols used in various contexts such as programming, mathematics, and formatting, often serving as operators, delimiters, or indicators of specific functions or relationships.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_087",
      "text": "Specialized tokens or symbols (like underscores, hyphens, angle brackets, or punctuation) used to denote technical constructs, identifiers, or formatting in code, markup, or scientific notation.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1735",
      "text": "The highlighted fragments are consistently short sub‑tokens that sit at the edges of larger identifiers or words—underscores, colons, arrows, or keyword fragments such as “IF” or “Bundle.” They function as syntactic or semantic delimiters, marking the start or end of a compound token or a special construct in code or text.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_911",
        "exp_087"
      ],
      "cosine_distance": 0.08689504008034377,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_911",
        "exp_1735"
      ],
      "cosine_distance": 0.12815078289078374,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_087",
        "exp_1735"
      ],
      "cosine_distance": 0.09326473450866535,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.58,
      "score_detection": 0.51,
      "score_simulation": 0.04014352541194061,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.66,
      "score_detection": 0.509090909090909,
      "score_simulation": 0.08631579289338147,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.64,
      "score_detection": 0.55,
      "score_simulation": 0.05526851209419853,
      "score_embedding": 0.6819999999999999
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.57,
      "score_detection": 0.5,
      "score_simulation": 0.11029706211040909,
      "score_embedding": 0.6708000000000001
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}