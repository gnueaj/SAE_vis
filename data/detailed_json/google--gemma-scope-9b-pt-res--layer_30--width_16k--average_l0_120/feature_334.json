{
  "feature_id": 334,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1105",
      "text": "Various linguistic and symbolic elements, including mathematical and scientific notation, programming syntax, and special characters, often used in specific contexts such as academic or technical writing, computer code, and data representation.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_281",
      "text": "High activation in tokens that represent structural or syntactic elements in text, including punctuation, spacing, formatting markers, and symbolic notation, often indicating boundaries, references, or formatting in code, mathematical expressions, or document markup.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1928",
      "text": "The highlighted segments are the model’s focus on semantically salient words or phrases—idiomatic expressions, comparative adjectives, key nouns, technical terms, or code tokens—whose activation values indicate their importance for the model’s processing.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1105",
        "exp_281"
      ],
      "cosine_similarity": 0.8762177811610722,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1105",
        "exp_1928"
      ],
      "cosine_similarity": 0.8322462062501256,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_281",
        "exp_1928"
      ],
      "cosine_similarity": 0.8806560107960308,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.33,
      "score_simulation": 0.14847141926567597,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5272727272727272,
      "score_detection": 0.28888888888888886,
      "score_simulation": 0.13354569425200388,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.24,
      "score_detection": 0.22,
      "score_simulation": 0.0561574909666374,
      "score_embedding": 0.4416
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.31,
      "score_detection": 0.27,
      "score_simulation": 0.017270393867823683,
      "score_embedding": 0.406
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.15,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}