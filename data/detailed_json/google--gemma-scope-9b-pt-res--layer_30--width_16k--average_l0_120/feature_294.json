{
  "feature_id": 294,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1071",
      "text": "Ellipses or other symbols used to indicate omission, continuation, or separation in text, often in formal or technical writing, mathematical notation, or programming code.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_247",
      "text": "The token sequences \\\"...\\\" and similar ellipsis-like patterns (e.g., \\\"....\\\", \\\"....\\\", \\\"⋯\\\", \\\"–\\\", \\\" \\\", \\\" \\\", \\\">>\\\", \\\"<<\\\", \\\" \\\\\\\\\\\", \\\" **\\\", \\\" --\\\", \\\" ⋅\\\", \\\" !\\\", \\\" *:\\\", \\\" *\\\", \\\" **\\\", \\\" ---\\\", \\\" …\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\\\\\\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\"",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1894",
      "text": "The highlighted tokens consistently serve as placeholders or markers for omitted, variable, or context‑dependent content—often expressed with ellipses, special symbols, or code‑style delimiters—across diverse textual settings.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1071",
        "exp_247"
      ],
      "cosine_distance": 0.12721608625061076,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1071",
        "exp_1894"
      ],
      "cosine_distance": 0.10808325599309143,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_247",
        "exp_1894"
      ],
      "cosine_distance": 0.12199796640573413,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8421052631578947,
      "score_detection": 0.79,
      "score_simulation": 0.3933721260905588,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.76,
      "score_detection": 0.7428571428571429,
      "score_simulation": 0.3767812045163803,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.85,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.89,
      "score_detection": 0.84,
      "score_simulation": 0.3498812667319713,
      "score_embedding": 0.5744
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.72,
      "score_detection": 0.7,
      "score_simulation": 0.6357870613467492,
      "score_embedding": 0.682
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.9,
      "score_detection": 0.925,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}