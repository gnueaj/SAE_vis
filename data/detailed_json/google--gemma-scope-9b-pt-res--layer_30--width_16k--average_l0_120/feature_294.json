{
  "feature_id": 294,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1071",
      "text": "Ellipses or other symbols used to indicate omission, continuation, or separation in text, often in formal or technical writing, mathematical notation, or programming code.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_247",
      "text": "The token sequences \\\"...\\\" and similar ellipsis-like patterns (e.g., \\\"....\\\", \\\"....\\\", \\\"⋯\\\", \\\"–\\\", \\\" \\\", \\\" \\\", \\\">>\\\", \\\"<<\\\", \\\" \\\\\\\\\\\", \\\" **\\\", \\\" --\\\", \\\" ⋅\\\", \\\" !\\\", \\\" *:\\\", \\\" *\\\", \\\" **\\\", \\\" ---\\\", \\\" …\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\\\\\\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\" \\\\<<\\\", \\\" \\\\>>\\\", \\\"",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1071",
        "exp_247"
      ],
      "cosine_distance": 0.12721608625061076,
      "euclidean_distance": 0.504412689964178
    }
  ],
  "scores": [
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.89,
      "score_detection": 0.84,
      "score_simulation": 0.3498812667319713,
      "score_embedding": 0.5744
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.72,
      "score_detection": 0.7,
      "score_simulation": 0.6357870613467492,
      "score_embedding": 0.682
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}