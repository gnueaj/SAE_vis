{
  "feature_id": 322,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1094",
      "text": "Words related to actions, interactions, and relationships between entities, often describing dynamic processes or connections.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_270",
      "text": "The suffix \\\"-ation\\\" is frequently used to form abstract nouns from verbs, often denoting processes or states, and is commonly activated in contexts involving actions, interactions, or systemic processes.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1917",
      "text": "The examples reveal a consistent focus on words that share a common morphological root—particularly the prefix “inter-” (and its counterpart “intra-”)—which signals relationships or internal states. The activations cluster around these prefixes and their derivations, indicating that the model is attuned to the semantic family of interaction, integration, and internality.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1094",
        "exp_270"
      ],
      "cosine_similarity": 0.8609757470976761,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1094",
        "exp_1917"
      ],
      "cosine_similarity": 0.8620972749890702,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_270",
        "exp_1917"
      ],
      "cosine_similarity": 0.8453648745343585,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.59,
      "score_detection": 0.44,
      "score_simulation": 0.257002680197855,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.76,
      "score_detection": 0.5,
      "score_simulation": 0.2520278365831974,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.74,
      "score_detection": 0.56,
      "score_simulation": 0.291753452685148,
      "score_embedding": 0.48560000000000003
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.72,
      "score_detection": 0.6,
      "score_simulation": 0.35977254667458375,
      "score_embedding": 0.5508000000000001
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}