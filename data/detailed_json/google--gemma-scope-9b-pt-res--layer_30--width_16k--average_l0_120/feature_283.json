{
  "feature_id": 283,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1062",
      "text": "Special characters, mathematical symbols, and units of measurement, often used in scientific or technical contexts, and sometimes used to separate values or indicate a range.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_238",
      "text": "The presence of non-alphabetic symbols, numerical digits, and special formatting elements such as subscripts, superscripts, and mathematical notation, often in scientific or technical contexts, with high activation on punctuation, spacing, and symbol clusters.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1885",
      "text": "The highlighted tokens are almost always content words that form a core phrase—nouns, adjectives, or numbers—often preceded by an article or preposition, and occasionally punctuation that marks a clause boundary. The pattern shows the model focusing on the semantic “building blocks” of a phrase rather than on function words alone.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1062",
        "exp_238"
      ],
      "cosine_similarity": 0.9133643864420212,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1062",
        "exp_1885"
      ],
      "cosine_similarity": 0.8042486492931483,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_238",
        "exp_1885"
      ],
      "cosine_similarity": 0.8414229693618682,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.13875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.36,
      "score_simulation": null,
      "score_embedding": 0.15960000000000002
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5142857142857142,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.21625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.21625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5714285714285714,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.15960000000000002
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.13875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.42,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.15960000000000002
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.37,
      "score_detection": 0.38,
      "score_simulation": null,
      "score_embedding": 0.13875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.575,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.21625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.9969255048242696,
      "z_score_fuzz": -1.3670476001435825,
      "z_score_detection": -0.811041394873527
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.881502668558148,
      "z_score_fuzz": -1.2608959310050782,
      "z_score_detection": -1.1503750882303452
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.5678957776959996,
      "z_score_fuzz": -0.7513679191402599,
      "z_score_detection": -0.6472251291150637
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.3916714999471262
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.4309245625978573
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.9888296086504411
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}