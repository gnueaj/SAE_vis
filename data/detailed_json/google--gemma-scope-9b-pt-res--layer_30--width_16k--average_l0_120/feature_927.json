{
  "feature_id": 927,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1584",
      "text": "Special characters, symbols, and punctuation marks, often used to denote code syntax, data types, or formatting in programming languages.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_760",
      "text": "Patterns involving punctuation, whitespace, and structural tokens in code or markup, particularly around syntax elements like braces, semicolons, parentheses, and delimiters, with emphasis on tokens that mark syntactic boundaries or formatting.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2407",
      "text": "The highlighted tokens are usually short, high‑frequency words or punctuation that perform a structural function—function words in idioms, comparative endings, or syntax symbols in code—marking boundaries or indicating containment or position.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1584",
        "exp_760"
      ],
      "cosine_similarity": 0.9038606687387507,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1584",
        "exp_2407"
      ],
      "cosine_similarity": 0.8618985743681432,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_760",
        "exp_2407"
      ],
      "cosine_similarity": 0.8808982646354381,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.675,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.44937499999999997
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.49,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.32999999999999996
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.4857142857142857,
      "score_simulation": null,
      "score_embedding": 0.34562500000000007
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.34562500000000007
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.32999999999999996
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.44937499999999997
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.37,
      "score_detection": 0.46,
      "score_simulation": null,
      "score_embedding": 0.32999999999999996
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.375,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.44937499999999997
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.34562500000000007
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.2773466791569292,
      "z_score_fuzz": -0.8983471533320341,
      "z_score_detection": -0.5302135107161605
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.9381908556206356,
      "z_score_fuzz": -1.0469594901259403,
      "z_score_detection": -0.6238228054352829
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.851692926764129,
      "z_score_fuzz": -0.1552854693625073,
      "z_score_detection": -0.6221512108867274
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.5686357810683745
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.8696577170606196
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.5430432023377879
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}