{
  "feature_id": 104,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_910",
      "text": "Nouns representing concepts, entities, or objects, often in formal or technical contexts, including mathematical, scientific, and legal terminology.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_086",
      "text": "The term \\\"factor\\\" and its variants (e.g., \\\"factors\\\", \\\"factorial\\\", \\\"fractional\\\") are frequently activated in mathematical and technical contexts involving divisibility, decomposition, or quantitative properties, often appearing in questions or statements about numbers, equations, or system parameters.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1734",
      "text": "The pattern seems to be that the important tokens are nouns or noun phrases that are often part of a larger phrase, sometimes with preceding words like \\\"the\\\", \\\"a\\\", etc.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_910",
        "exp_086"
      ],
      "cosine_distance": 0.15444782573574078,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_910",
        "exp_1734"
      ],
      "cosine_distance": 0.1387548258737712,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_086",
        "exp_1734"
      ],
      "cosine_distance": 0.1937495050151642,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.63,
      "score_detection": 0.57,
      "score_simulation": 0.23832485618432564,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6909090909090909,
      "score_detection": 0.6,
      "score_simulation": 0.2483405551816495,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7368421052631579,
      "score_detection": 0.5789473684210527,
      "score_simulation": 0.17965624599397295,
      "score_embedding": 0.29000000000000004
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.631578947368421,
      "score_simulation": 0.3517852469234497,
      "score_embedding": 0.5359999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.85,
      "score_detection": 0.75,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}