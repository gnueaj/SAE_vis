{
  "feature_id": 407,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1159",
      "text": "Punctuation marks and special characters, often used to denote the end of a statement, separate code blocks, or indicate the start of a new line.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_335",
      "text": "The presence of special tokens or symbols (like punctuation, delimiters, or structural markers) that often appear in code, markup, or formatted text, frequently associated with syntax, structure, or tokenization boundaries.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1982",
      "text": "The highlighted tokens are the syntactic delimiters and key identifiers that define the structure of code fragmentsâ€”punctuation, braces, semicolons, parentheses, and language keywords or variable names that mark boundaries or essential elements.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1159",
        "exp_335"
      ],
      "cosine_distance": 0.08926503158854082,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1159",
        "exp_1982"
      ],
      "cosine_distance": 0.12218349902020709,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_335",
        "exp_1982"
      ],
      "cosine_distance": 0.08520414933622067,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5428571428571428,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.58,
      "score_detection": 0.53,
      "score_simulation": 0.053348811158699584,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.55,
      "score_detection": 0.49473684210526314,
      "score_simulation": 0.05953418838222561,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.575,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.63,
      "score_detection": 0.65,
      "score_simulation": 0.0524137757494155,
      "score_embedding": 0.4648
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.59,
      "score_detection": 0.61,
      "score_simulation": 0.06399823447253866,
      "score_embedding": 0.5428
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}