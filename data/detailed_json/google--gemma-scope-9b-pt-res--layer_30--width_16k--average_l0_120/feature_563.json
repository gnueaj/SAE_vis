{
  "feature_id": 563,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1287",
      "text": "Code snippets in various programming languages, often containing method calls, variable assignments, and conditional statements, with a focus on syntax and structure.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_463",
      "text": "Repeated use of delimiters enclosing identifiers or code elements, often surrounding variable names, method calls, or structural tokens in programming syntax, with high activation on the enclosed tokens and surrounding whitespace or punctuation.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2110",
      "text": "The highlighted tokens are identifiers or keywords that play a key role in the code’s logic—function names, variable names, class names, or control‑flow keywords—often followed by syntax such as parentheses, dots, or braces that signals their use. The activation scores indicate how central each token is to the surrounding code.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1287",
        "exp_463"
      ],
      "cosine_similarity": 0.8734514625061238,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1287",
        "exp_2110"
      ],
      "cosine_similarity": 0.878322635731413,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_463",
        "exp_2110"
      ],
      "cosine_similarity": 0.9038302512011542,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.499375
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.49,
      "score_detection": 0.44,
      "score_simulation": null,
      "score_embedding": 0.48
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.64,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.379375
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.379375
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.48
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.499375
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.49,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.48
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.499375
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.379375
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.0005533068161094502,
      "z_score_fuzz": -1.3556158819286663,
      "z_score_detection": -0.34299492127791603
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.10781073859817716,
      "z_score_fuzz": -1.0583912083408555,
      "z_score_detection": -0.6238228054352829
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.6648574004340762,
      "z_score_fuzz": -0.6925762254635495,
      "z_score_detection": -0.35469608311780654
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.5663880366742305
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.5966749174581052
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.570709903005144
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}