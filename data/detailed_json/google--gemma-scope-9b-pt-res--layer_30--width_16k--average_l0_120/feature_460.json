{
  "feature_id": 460,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1203",
      "text": "Prefixes or suffixes of words, often indicating a relationship or a characteristic, and sometimes part of proper nouns or abbreviations.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_379",
      "text": "Partial or truncated proper nouns, abbreviations, or technical terms often appear with lowercase or fragmented spelling, typically preceding or following a contextually significant word or symbol.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2026",
      "text": "The model highlights subword fragments that are part of larger identifiers or words, often at the start or end, indicating sensitivity to morphological or code token boundaries.\\\" Also mention that many tokens are prefixes like \\\"NS\\\", \\\"ns\\\", \\\"Ins\\\", \\\"ens\\\", \\\"ensuring\\\", \\\"ensure\\\", \\\"insured\\\", \\\"insurance\\\", \\\"inscribed\\\", \\\"insulin\\\", \\\"ins\\\", \\\"NSLog\\\", \\\"namespace\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", \\\"NS\\\", ",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1203",
        "exp_379"
      ],
      "cosine_similarity": 0.8840891272146119,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1203",
        "exp_2026"
      ],
      "cosine_similarity": 0.8555328937291765,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_379",
        "exp_2026"
      ],
      "cosine_similarity": 0.8612367574172628,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.86875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.74,
      "score_detection": 0.68,
      "score_simulation": null,
      "score_embedding": 0.8204
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.7142857142857143,
      "score_simulation": null,
      "score_embedding": 0.9125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.9125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.675,
      "score_detection": 0.6857142857142857,
      "score_simulation": null,
      "score_embedding": 0.8204
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": 0.86875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.78,
      "score_detection": 0.72,
      "score_simulation": null,
      "score_embedding": 0.8204
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7684210526315789,
      "score_detection": 0.6736842105263158,
      "score_simulation": null,
      "score_embedding": 0.86875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.8,
      "score_detection": 0.775,
      "score_simulation": null,
      "score_embedding": 0.9125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 2.0442577313516943,
      "z_score_fuzz": 0.7442102533374466,
      "z_score_detection": 0.40280018335930173
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 1.776598540298122,
      "z_score_fuzz": 0.5763444963921048,
      "z_score_detection": 0.9575056374984595
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 2.286451932149911,
      "z_score_fuzz": 0.7592519878307573,
      "z_score_detection": 0.848851991842336
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 1.0637560560161474
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 1.1034828913962287
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 1.2981853039410014
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}