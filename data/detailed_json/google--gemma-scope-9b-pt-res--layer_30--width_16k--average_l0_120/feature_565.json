{
  "feature_id": 565,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1289",
      "text": "A token that is often used to denote a shift or a change in the text, such as a greater-than or less-than symbol, a left or right arrow, or an asterisk, and is frequently used in programming languages, mathematical expressions, and citations.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_465",
      "text": "Patterns in the text involve structural or syntactic elements such as line breaks, punctuation, and formatting markers (e.g., newlines, parentheses, brackets, and special symbols) that often appear in code, markup, or formatted documents, with activations concentrated on delimiters, whitespace, and structural tokens.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2112",
      "text": "The highlighted tokens are primarily structural delimiters—punctuation, brackets, and whitespace—that mark boundaries or key positions in the text or code.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1289",
        "exp_465"
      ],
      "cosine_distance": 0.14196182117631484,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1289",
        "exp_2112"
      ],
      "cosine_distance": 0.141244955781754,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_465",
        "exp_2112"
      ],
      "cosine_distance": 0.08634695978752227,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.48,
      "score_detection": 0.41,
      "score_simulation": -0.02246448342148417,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5777777777777777,
      "score_detection": 0.475,
      "score_simulation": -0.027570283403805733,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.53,
      "score_detection": 0.44,
      "score_simulation": -0.018979696397390347,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.44,
      "score_detection": 0.42,
      "score_simulation": 0.20173415367678377,
      "score_embedding": 0.6232
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}