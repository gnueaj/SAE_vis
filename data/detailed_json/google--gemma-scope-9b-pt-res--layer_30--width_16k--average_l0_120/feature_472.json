{
  "feature_id": 472,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1212",
      "text": "Possessive pronouns, articles, and other function words, often used to indicate ownership or association, and sometimes preceding a quotation or a proper noun.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_388",
      "text": "Possessive pronouns like \\\"his\\\", \\\"their\\\", \\\"your\\\" and demonstratives like \\\"this\\\", \\\"that\\\" are frequently activated when referring to previously mentioned entities, often in contexts involving personal attribution, possession, or contextual reference.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2035",
      "text": "The highlighted tokens are short, high‑frequency function words—pronouns, determiners, prepositions, conjunctions, or small nouns—that signal grammatical relationships such as possession, reference, or clause linkage. They are crucial for structuring the sentence and connecting ideas.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1212",
        "exp_388"
      ],
      "cosine_similarity": 0.8882401023556389,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1212",
        "exp_2035"
      ],
      "cosine_similarity": 0.8819252664706171,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_388",
        "exp_2035"
      ],
      "cosine_similarity": 0.852187523839554,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.396875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.56,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.41759999999999997
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.45714285714285713,
      "score_simulation": null,
      "score_embedding": 0.369375
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.369375
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6571428571428571,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.41759999999999997
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.396875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.68,
      "score_detection": 0.38,
      "score_simulation": null,
      "score_embedding": 0.41759999999999997
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.396875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.369375
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.5679797201147896,
      "z_score_fuzz": 0.016190303861229847,
      "z_score_detection": -0.3663972449576963
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.4532488672795199,
      "z_score_fuzz": -0.10465928869645165,
      "z_score_detection": -0.764236747513966
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.72021607490224,
      "z_score_fuzz": -0.1552854693625073,
      "z_score_detection": -0.6890149928289573
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.306062220403752
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.44071496782997915
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.5215055123645682
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}