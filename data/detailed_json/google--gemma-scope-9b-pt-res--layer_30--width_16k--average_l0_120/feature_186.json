{
  "feature_id": 186,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_981",
      "text": "Adjectives and nouns describing scope, scale, or geography, often indicating something that is widespread, comprehensive, or related to the entire world.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_157",
      "text": "The word \\\"global\\\" and its variants (e.g., \\\"world\\\", \\\"global\\\", \\\"worldwide\\\") are frequently used to denote broad, widespread, or universal scope, often in contexts involving geography, scale, or systemic impact.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1805",
      "text": "The highlighted tokens are mainly nouns or adjectives that denote places, objects, or abstract concepts, often capitalized or part of proper names, and they serve as key semantic anchors across diverse contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_981",
        "exp_157"
      ],
      "cosine_similarity": 0.9417330397004731,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_981",
        "exp_1805"
      ],
      "cosine_similarity": 0.8387349025570289,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_157",
        "exp_1805"
      ],
      "cosine_similarity": 0.8225881099456136,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.81,
      "score_detection": 0.76,
      "score_simulation": 0.4674633333404247,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7428571428571429,
      "score_detection": 0.7647058823529411,
      "score_simulation": 0.6026039256719791,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.81,
      "score_detection": 0.78,
      "score_simulation": 0.5493641419744265,
      "score_embedding": 0.804
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.78,
      "score_detection": 0.73,
      "score_simulation": null,
      "score_embedding": 0.8156
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}