{
  "feature_id": 880,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1546",
      "text": "Various parts of speech, including nouns, verbs, adjectives, adverbs, and function words, often in the context of descriptive or informative writing, and sometimes in the context of dialogue or technical writing.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_722",
      "text": "Commonly activated tokens include function words (e.g., \\\"and\\\", \\\"is\\\", \\\"has\\\", \\\"of\\\"), possessive or descriptive modifiers (e.g., \\\"special\\\", \\\"considerable\\\", \\\"meticulously\\\"), and compound or multi-token concepts (e.g., \\\"thought and care\\\", \\\"a considerable portion of\\\") that convey effort, quality, or detail in a descriptive or evaluative context.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2369",
      "text": "The highlighted tokens are the individual words that make up the core of a phrase or clause that is being emphasized or singled out in the surrounding text. They tend to be content words (nouns, verbs, adjectives, adverbs) or key function words that anchor the phrase, and the activations indicate their relative importance within that phrase. The pattern is that each highlighted segment is a contiguous linguistic unit—often a noun phrase, verb phrase, or idiomatic expression—whose constituent words are marked for their contribution to the meaning or emphasis of the sentence.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1546",
        "exp_722"
      ],
      "cosine_similarity": 0.8597902530659391,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1546",
        "exp_2369"
      ],
      "cosine_similarity": 0.846175139213476,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_722",
        "exp_2369"
      ],
      "cosine_similarity": 0.8734028545880804,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.42857142857142855,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.38187499999999996
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.49,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.42800000000000005
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.44,
      "score_detection": 0.43333333333333335,
      "score_simulation": null,
      "score_embedding": 0.288125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.3,
      "score_detection": 0.4857142857142857,
      "score_simulation": null,
      "score_embedding": 0.288125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.42800000000000005
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4857142857142857,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.38187499999999996
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.42,
      "score_simulation": null,
      "score_embedding": 0.42800000000000005
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.38187499999999996
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.325,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.288125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.6510177318170355,
      "z_score_fuzz": -1.437271012035208,
      "z_score_detection": -0.6472251291150637
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.39567584583262905,
      "z_score_fuzz": -1.035527771911024,
      "z_score_detection": -0.6823286146347342
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.1700053049560717,
      "z_score_fuzz": -2.0072238201788672,
      "z_score_detection": -0.8366725112847151
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.911837957655769
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.7045107441261291
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -1.3379672121398845
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}