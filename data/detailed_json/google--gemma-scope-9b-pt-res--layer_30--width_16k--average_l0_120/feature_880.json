{
  "feature_id": 880,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1546",
      "text": "Various parts of speech, including nouns, verbs, adjectives, adverbs, and function words, often in the context of descriptive or informative writing, and sometimes in the context of dialogue or technical writing.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_722",
      "text": "Various parts of speech, including nouns, verbs, adjectives, adverbs, and function words, often in the context of descriptive or informative writing, and sometimes in the context of dialogue or technical writing.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2369",
      "text": "The highlighted tokens are the individual words that make up the core of a phrase or clause that is being emphasized or singled out in the surrounding text. They tend to be content words (nouns, verbs, adjectives, adverbs) or key function words that anchor the phrase, and the activations indicate their relative importance within that phrase. The pattern is that each highlighted segment is a contiguous linguistic unit—often a noun phrase, verb phrase, or idiomatic expression—whose constituent words are marked for their contribution to the meaning or emphasis of the sentence.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1546",
        "exp_722"
      ],
      "cosine_distance": 0.0,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1546",
        "exp_2369"
      ],
      "cosine_distance": 0.15382486078652402,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_722",
        "exp_2369"
      ],
      "cosine_distance": 0.15382486078652402,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.42857142857142855,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.49,
      "score_detection": 0.49,
      "score_simulation": 0.036938806361605105,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6166666666666667,
      "score_detection": 0.5473684210526316,
      "score_simulation": 0.03620634563946206,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4857142857142857,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.42,
      "score_simulation": -0.009754765526899295,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4,
      "score_detection": 0.425,
      "score_simulation": -0.03247445925803142,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.325,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}