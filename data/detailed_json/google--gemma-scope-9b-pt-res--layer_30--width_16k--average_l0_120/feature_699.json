{
  "feature_id": 699,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1403",
      "text": "Numerical values, often embedded within text, and sometimes used as part of a larger numerical expression or identifier.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_579",
      "text": "The digit '5' and the sequence 'th' frequently appear in ordinal numbers, often in contexts involving dates, centuries, or ranked positions, with high activation values indicating their importance in identifying numerical or ordinal patterns.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2226",
      "text": "The highlighted tokens are the core lexical elements of idiomatic or figurative expressions, the comparative suffix “‑er”, or nouns that denote a container or location, often occurring adjacent to quotation marks or numeric contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1403",
        "exp_579"
      ],
      "cosine_similarity": 0.8635665703269338,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1403",
        "exp_2226"
      ],
      "cosine_similarity": 0.8626482207427135,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_579",
        "exp_2226"
      ],
      "cosine_similarity": 0.8326806260183642,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.525,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.728125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.53,
      "score_simulation": null,
      "score_embedding": 0.8300000000000001
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.8256249999999999
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.8256249999999999
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.55,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.8300000000000001
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.728125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.71,
      "score_detection": 0.58,
      "score_simulation": null,
      "score_embedding": 0.8300000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.52,
      "score_detection": 0.36,
      "score_simulation": null,
      "score_embedding": 0.728125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.575,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.8256249999999999
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.2657763716431396,
      "z_score_fuzz": -0.45251014295031816,
      "z_score_detection": -1.1503750882303452
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 1.8297428677875596,
      "z_score_fuzz": -0.07526344185809639,
      "z_score_detection": -0.2142821410391231
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.805523447707737,
      "z_score_fuzz": -0.5553956068845605,
      "z_score_detection": -0.764236747513966
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.11236961984584126
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.5133990949634467
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.16196369776973688
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}