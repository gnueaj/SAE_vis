{
  "feature_id": 995,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1644",
      "text": "Adjectives and adverbs describing positive qualities, often used to describe services, staff, or experiences, as well as nouns and verbs related to hospitality and customer interactions.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_820",
      "text": "Commonly activated tokens include function words like \\\"a\\\", \\\"and\\\", \\\"of\\\", \\\"with\\\", \\\"for\\\", \\\"are\\\", \\\"was\\\", \\\"were\\\", \\\"very\\\", \\\"friendly\\\", \\\"time\\\", \\\"erred\\\", \\\"relaxed\\\", \\\"wealthy\\\", \\\"open\\\", \\\"mic\\\", \\\"Tail\\\", \\\"Warm\\\", \\\"seriously\\\", and \\\"over\\\" in context, often appearing in descriptive or evaluative phrases related to services, locations, or attributes, particularly in reviews or travel-related text.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2467",
      "text": "The highlighted fragments are almost always short, high‑frequency function words that act as grammatical glue or modifiers within key phrases—often preceding or following a noun or adjective to convey description, sentiment, or relational meaning.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1644",
        "exp_820"
      ],
      "cosine_similarity": 0.8552820861422789,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1644",
        "exp_2467"
      ],
      "cosine_similarity": 0.8015421200113453,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_820",
        "exp_2467"
      ],
      "cosine_similarity": 0.8495478898186405,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4,
      "score_detection": 0.25,
      "score_simulation": null,
      "score_embedding": 0.20874999999999996
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.39,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": 0.2868
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.339375
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.339375
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.34285714285714286,
      "score_detection": 0.2,
      "score_simulation": null,
      "score_embedding": 0.2868
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.225,
      "score_simulation": null,
      "score_embedding": 0.20874999999999996
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.36,
      "score_detection": 0.26,
      "score_simulation": null,
      "score_embedding": 0.2868
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.275,
      "score_detection": 0.175,
      "score_simulation": null,
      "score_embedding": 0.20874999999999996
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.339375
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.6094147835471226,
      "z_score_fuzz": -1.9843603837490351,
      "z_score_detection": -2.402399405098605
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.1773403293231035,
      "z_score_fuzz": -1.9435328186957646,
      "z_score_detection": -2.1449738446210187
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.886292098306732,
      "z_score_fuzz": -0.2696026515116649,
      "z_score_detection": -0.5302135107161605
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.9987248574649208
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.755282330879962
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.5620360868448525
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}