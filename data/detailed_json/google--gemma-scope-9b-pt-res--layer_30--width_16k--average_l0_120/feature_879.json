{
  "feature_id": 879,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1545",
      "text": "The word \\\"all\\\" is often used to emphasize inclusivity or totality, and is sometimes used in formal or legal contexts to convey a sense of universality or completeness.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_721",
      "text": "The word \\\"all\\\" is often used to emphasize inclusivity or totality, and is sometimes used in formal or legal contexts to convey a sense of universality or completeness.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2368",
      "text": "The highlighted words are universal determiners or pronouns—primarily “All” (capitalized or lowercase) and “every”—that signal totality or inclusiveness. They appear as standalone tokens or within common phrases such as “All of,” “All the,” “All rights reserved,” and “every solution.” These terms are used to refer to an entire set or group, often in formal, legal, or descriptive contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1545",
        "exp_721"
      ],
      "cosine_similarity": 1.0,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1545",
        "exp_2368"
      ],
      "cosine_similarity": 0.9231393856017636,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_721",
        "exp_2368"
      ],
      "cosine_similarity": 0.9231393856017636,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.85,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.9,
      "score_detection": 0.88,
      "score_simulation": 0.7717767298924507,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.88,
      "score_detection": 0.8,
      "score_simulation": 0.8692749110371606,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.9,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.94,
      "score_detection": 0.9,
      "score_simulation": 0.437871271517029,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.875,
      "score_detection": 0.8,
      "score_simulation": 0.5465100688401193,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.975,
      "score_detection": 0.85,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}