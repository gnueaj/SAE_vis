{
  "feature_id": 553,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1277",
      "text": "Titles of books, series, chapters, and other written works, often including subtitles, authors, and publication information.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_453",
      "text": "Common patterns include proper nouns, titles, and compound terms, often involving book, series, or fictional work names, with frequent use of underscores, hyphens, and capitalization to denote titles or special terms.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2100",
      "text": "The highlighted tokens are the core lexical elements that compose named entities, titles, or key phrases, often split by tokenization, and they carry the main semantic content of the surrounding text.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1277",
        "exp_453"
      ],
      "cosine_similarity": 0.8750554393268432,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1277",
        "exp_2100"
      ],
      "cosine_similarity": 0.801017930705667,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_453",
        "exp_2100"
      ],
      "cosine_similarity": 0.8346585249031022,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.46312500000000006
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.62,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.4144
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5714285714285714,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.5225
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.5225
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.4144
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.46312500000000006
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5263157894736842,
      "score_detection": 0.52,
      "score_simulation": null,
      "score_embedding": 0.4144
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.54,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.46312500000000006
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.575,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.5225
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}