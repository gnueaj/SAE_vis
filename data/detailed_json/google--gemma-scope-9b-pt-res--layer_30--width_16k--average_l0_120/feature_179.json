{
  "feature_id": 179,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_975",
      "text": "Various symbols and characters used in programming languages, mathematical expressions, and formatting, often serving as operators, delimiters, or indicators of specific functions or structures.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_151",
      "text": "The presence of special characters, symbols, or syntax elements used in programming, markup, or mathematical notation, often indicating structural or syntactic components in code, documentation, or formal expressions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1799",
      "text": "The highlighted tokens are contiguous sequences that together form a meaningful phrase or code fragment, often representing the core content of the sentence or snippet.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_975",
        "exp_151"
      ],
      "cosine_similarity": 0.929188606117909,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_975",
        "exp_1799"
      ],
      "cosine_similarity": 0.8177721490507666,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_151",
        "exp_1799"
      ],
      "cosine_similarity": 0.8400734376498342,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.49375
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.44,
      "score_detection": 0.54,
      "score_simulation": null,
      "score_embedding": 0.4372
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7333333333333333,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.4425
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.4425
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.4372
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.49375
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.47,
      "score_detection": 0.58,
      "score_simulation": null,
      "score_embedding": 0.4372
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.46,
      "score_detection": 0.59,
      "score_simulation": null,
      "score_embedding": 0.49375
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": 0.4425
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.03169256120445159,
      "z_score_fuzz": -1.0469594901259403,
      "z_score_detection": -0.3780984067975869
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.34474586532191864,
      "z_score_fuzz": -1.2755938544242558,
      "z_score_detection": -0.3663972449576963
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.31540576785379165,
      "z_score_fuzz": -0.364866969969297,
      "z_score_detection": -0.6472251291150637
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.48558348604265955
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.6622456549012902
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.4424992889793841
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}