{
  "feature_id": 488,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1226",
      "text": "Various scientific and technical terms, including names of plants, biological processes, and chemical compounds, as well as programming-related terms and symbols.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_402",
      "text": "Fragments of compound words or scientific terms, often derived from Latin or Greek roots, where the activation focuses on partial morphemes that contribute to the meaning of the full term.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2049",
      "text": "The examples reveal that the model’s activations focus on sub‑word fragments—prefixes, suffixes, or internal pieces of longer words—rather than on whole words. These fragments often come from domain‑specific terminology and are repeated across different contexts, indicating that the model relies on a sub‑word tokenization scheme (e.g., BPE or WordPiece) to represent and process rare or complex lexical items.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1226",
        "exp_402"
      ],
      "cosine_similarity": 0.8483804758228434,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1226",
        "exp_2049"
      ],
      "cosine_similarity": 0.8309252880232542,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_402",
        "exp_2049"
      ],
      "cosine_similarity": 0.8851390675014938,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.43,
      "score_detection": 0.42,
      "score_simulation": 0.04957730017391598,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4375,
      "score_detection": 0.3875,
      "score_simulation": 0.09725304354301895,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4421052631578947,
      "score_detection": 0.39,
      "score_simulation": null,
      "score_embedding": 0.3608
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.49,
      "score_detection": 0.28,
      "score_simulation": 0.16996456911978752,
      "score_embedding": 0.36719999999999997
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.4,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}