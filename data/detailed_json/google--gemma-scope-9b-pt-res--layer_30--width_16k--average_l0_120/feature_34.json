{
  "feature_id": 34,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_853",
      "text": "Punctuation marks and common function words, often used to connect clauses or phrases, or to indicate possession, existence, or comparison.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_029",
      "text": "Common function words and short phrases that serve grammatical or structural roles in sentences, often appearing in contexts involving comparison, possession, location, or logical connection.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1677",
      "text": "The highlighted tokens are consistently short, high‑frequency words or essential content words that anchor the main noun or verb phrase in each sentence, indicating that the model focuses on the core semantic units that carry the sentence’s meaning.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_853",
        "exp_029"
      ],
      "cosine_similarity": 0.9346071177708836,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_853",
        "exp_1677"
      ],
      "cosine_similarity": 0.8088577224390053,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_029",
        "exp_1677"
      ],
      "cosine_similarity": 0.8405403205507782,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.39437500000000003
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.43,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.34559999999999996
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.375,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": 0.37
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.3,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.37
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5142857142857142,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.34559999999999996
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.375,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.39437500000000003
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.52,
      "score_detection": 0.43,
      "score_simulation": null,
      "score_embedding": 0.34559999999999996
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.45,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.39437500000000003
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.3,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.37
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.5818193887318303,
      "z_score_fuzz": -1.6414088373015612,
      "z_score_detection": -1.2907890303090286
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.8518313234503,
      "z_score_fuzz": -1.094319465587734,
      "z_score_detection": -0.8812483659128687
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.7167561577479798,
      "z_score_fuzz": -2.2129947480473513,
      "z_score_detection": -1.75883550390464
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.1713390854474734
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.9424663849836342
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -1.5628621365666568
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}