{
  "feature_id": 452,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1195",
      "text": "Numerical digits embedded within text, often used to represent quantities, measurements, or codes.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_371",
      "text": "The digit \\\"6\\\" appears frequently in numerical contexts, often as part of multi-digit numbers, dates, or measurements, and is sometimes associated with specific formatting patterns like decimals or ranges. Other digits like \\\"2\\\", \\\"8\\\", and \\\"0\\\" also appear in similar numerical contexts, but \\\"6\\\" stands out as a recurring activation point in various numeric sequences.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2018",
      "text": "The highlighted tokens are numeric digits that appear within numeric values in the text.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1195",
        "exp_371"
      ],
      "cosine_similarity": 0.8773403252066425,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1195",
        "exp_2018"
      ],
      "cosine_similarity": 0.9129770350738257,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_371",
        "exp_2018"
      ],
      "cosine_similarity": 0.8748461632082223,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.85,
      "score_simulation": null,
      "score_embedding": 0.46124999999999994
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.96,
      "score_detection": 0.86,
      "score_simulation": null,
      "score_embedding": 0.45840000000000003
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 1.0,
      "score_detection": 0.8571428571428571,
      "score_simulation": null,
      "score_embedding": 0.286875
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.95,
      "score_detection": 0.875,
      "score_simulation": null,
      "score_embedding": 0.286875
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.875,
      "score_detection": 0.85,
      "score_simulation": null,
      "score_embedding": 0.45840000000000003
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.925,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": 0.46124999999999994
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 1.0,
      "score_detection": 0.88,
      "score_simulation": null,
      "score_embedding": 0.45840000000000003
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.84,
      "score_detection": 0.83,
      "score_simulation": null,
      "score_embedding": 0.46124999999999994
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.975,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": 0.286875
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.2116082532259847,
      "z_score_fuzz": 1.422291644295873,
      "z_score_detection": 1.938731637500688
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.2273854754494109,
      "z_score_fuzz": 2.039604427901326,
      "z_score_detection": 2.1376513887788224
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.1769251392645923,
      "z_score_fuzz": 2.2453753557698106,
      "z_score_detection": 2.0022522303458077
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 1.0498050095235254
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 1.3166234470769125
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 1.0235674822836753
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}