{
  "feature_id": 390,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1147",
      "text": "Punctuation marks, often used to denote the end of a sentence, quotation, or code block, and sometimes used in conjunction with other symbols to indicate a specific formatting or syntax.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_323",
      "text": "Closing punctuation and bracketing symbols, particularly quotation marks, brackets, and braces, are frequently activated when they appear at the end of a sequence, often signaling the end of a quoted or structured text segment.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1970",
      "text": "The markers isolate contiguous groups of tokens that are semantically or syntactically significant—often idiomatic phrases, comparative adjectives, or code‑style fragments—whose individual words or punctuation are highlighted by the activation list.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1147",
        "exp_323"
      ],
      "cosine_similarity": 0.8966924067108633,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1147",
        "exp_1970"
      ],
      "cosine_similarity": 0.8443002909580307,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_323",
        "exp_1970"
      ],
      "cosine_similarity": 0.8474663022670597,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8666666666666667,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.5256250000000001
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.61,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.7048
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.37142857142857144,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.6287499999999999
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.4,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.6287499999999999
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.7048
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.825,
      "score_detection": 0.875,
      "score_simulation": null,
      "score_embedding": 0.5256250000000001
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.89,
      "score_detection": 0.71,
      "score_simulation": null,
      "score_embedding": 0.7048
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.83,
      "score_detection": 0.91,
      "score_simulation": null,
      "score_embedding": 0.5256250000000001
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.425,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.6287499999999999
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.14476321366282136,
      "z_score_fuzz": 1.323216753099936,
      "z_score_detection": 1.8334211809416754
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 1.1366522634461471,
      "z_score_fuzz": 0.35914185030870416,
      "z_score_detection": 0.07824690495813359
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.7156495441157604,
      "z_score_fuzz": -1.7067329413867949,
      "z_score_detection": -1.4078006487079309
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 1.100467049234811
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.524680339570995
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.7996280153263218
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}