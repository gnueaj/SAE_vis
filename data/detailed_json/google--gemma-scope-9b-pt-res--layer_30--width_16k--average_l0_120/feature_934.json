{
  "feature_id": 934,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1588",
      "text": "Punctuation marks and special characters separating or ending sentences, clauses, or phrases, often indicating a pause or a shift in thought.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_764",
      "text": "The presence of whitespace tokens (spaces, tabs, or line breaks) and punctuation marks (like hyphens, quotation marks, or ellipses) that serve as structural or syntactic separators in text, often indicating formatting, emphasis, or linguistic boundaries.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2411",
      "text": "The highlighted tokens are the core lexical or sub‑lexical units that carry the main semantic load of a phrase—typically nouns, adjectives, or comparative suffixes—often demarcated by punctuation or spacing that signals a boundary.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1588",
        "exp_764"
      ],
      "cosine_similarity": 0.899387684795509,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1588",
        "exp_2411"
      ],
      "cosine_similarity": 0.8482644617685956,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_764",
        "exp_2411"
      ],
      "cosine_similarity": 0.8768960242231393,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5714285714285714,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.185
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.1572
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4666666666666667,
      "score_detection": 0.4666666666666667,
      "score_simulation": null,
      "score_embedding": 0.10125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.25,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.10125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.525,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.1572
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.725,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.185
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.61,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.1572
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.185
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.3,
      "score_detection": 0.25,
      "score_simulation": null,
      "score_embedding": 0.10125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.7408916354090118,
      "z_score_fuzz": 0.008024790850574972,
      "z_score_detection": -0.06216703712054986
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.8947887504305074,
      "z_score_fuzz": -0.24673921508183355,
      "z_score_detection": -0.12067284632000105
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -2.2045205340798844,
      "z_score_fuzz": -2.11773042958972,
      "z_score_detection": -1.1347735391104916
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.5983446272263289
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.7540669372774474
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -1.8190081675933654
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}