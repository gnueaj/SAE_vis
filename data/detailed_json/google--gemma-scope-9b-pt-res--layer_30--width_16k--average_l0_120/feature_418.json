{
  "feature_id": 418,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1168",
      "text": "Scientific and technical terms, often representing specific concepts, species, or biological processes, typically in the fields of genetics, biology, and medicine.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_344",
      "text": "Fragments of scientific terminology, particularly genetic, biological, and molecular biology terms, often appear as partial tokens (e.g., \\\"hetero\\\", \\\"zyg\\\", \\\"phenotyp\\\", \\\"genetic\\\") and are frequently associated with comparative, structural, or functional descriptions in academic text.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1991",
      "text": "The highlighted tokens are sub‑word fragments that belong to domain‑specific biological terminology (e.g., “homo”, “hetero”, “allele”, “gene”, “genome”, “wild”, “asymmetric”, “coding”, “phenotyp”), showing that the model focuses on biologically meaningful sub‑word units rather than whole words.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1168",
        "exp_344"
      ],
      "cosine_distance": 0.10390321184719764,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1168",
        "exp_1991"
      ],
      "cosine_distance": 0.1242150244381961,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_344",
        "exp_1991"
      ],
      "cosine_distance": 0.09707810903499281,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.75,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.53,
      "score_detection": 0.48,
      "score_simulation": 0.15582241713979078,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.725,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6666666666666666,
      "score_detection": 0.4842105263157895,
      "score_simulation": 0.198644914495317,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.66,
      "score_detection": 0.47,
      "score_simulation": 0.044609390419020296,
      "score_embedding": 0.41480000000000006
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.64,
      "score_detection": 0.5,
      "score_simulation": 0.271210286039792,
      "score_embedding": 0.3016
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.725,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}