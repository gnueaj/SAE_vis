{
  "feature_id": 136,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_936",
      "text": "Prepositions and conjunctions, often used to connect clauses or phrases, and sometimes function words like articles, auxiliary verbs, and adverbs, which provide grammatical structure and context.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_112",
      "text": "Prepositions and particles (e.g., \\\"in\\\", \\\"on\\\", \\\"of\\\", \\\"to\\\", \\\"with\\\", \\\"for\\\", \\\"at\\\", \\\"by\\\") frequently appear in contextually critical positions, often linking clauses, modifying nouns, or introducing relationships between entities, with high activation values indicating their syntactic and semantic importance in sentence structure.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1760",
      "text": "The highlighted tokens are the words that together form a phrase—usually a prepositional or verb phrase—that is semantically relevant to the surrounding context; they tend to be function words or short content words that anchor that phrase.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_936",
        "exp_112"
      ],
      "cosine_similarity": 0.8962130255553901,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_936",
        "exp_1760"
      ],
      "cosine_similarity": 0.8679455952688468,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_112",
        "exp_1760"
      ],
      "cosine_similarity": 0.8717045040892839,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.67,
      "score_detection": 0.49,
      "score_simulation": 0.0790642691187754,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.325,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6333333333333333,
      "score_detection": 0.49230769230769234,
      "score_simulation": 0.10077605353902186,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.73,
      "score_detection": 0.53,
      "score_simulation": 0.10022922042766703,
      "score_embedding": 0.6464
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.68,
      "score_detection": 0.44,
      "score_simulation": null,
      "score_embedding": 0.7063999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}