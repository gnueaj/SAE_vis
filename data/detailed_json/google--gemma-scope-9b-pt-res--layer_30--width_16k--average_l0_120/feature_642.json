{
  "feature_id": 642,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1355",
      "text": "Punctuation and special characters, often used to denote mathematical or programming syntax, or to indicate a specific formatting or emphasis in text.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_531",
      "text": "Common patterns include the use of comparative or superlative forms, technical or domain-specific terminology, and structural markers in code or mathematical notation, often involving suffixes like \\\"er\\\", \\\"max\\\", \\\"min\\\", or symbols like \\\">\\\", \\\":\\\", and \\\"Â±\\\", with frequent emphasis on precision, measurement, or logical structure.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2178",
      "text": "The highlighted tokens are the core content words or technical keywords that carry the main meaning of each phrase or code fragment. They are typically nouns, adjectives, or programming terms that serve as the subject, object, or key concept, and they are often surrounded by punctuation or whitespace that marks the boundaries of the phrase.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1355",
        "exp_531"
      ],
      "cosine_similarity": 0.8766806215237622,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1355",
        "exp_2178"
      ],
      "cosine_similarity": 0.8419486696828211,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_531",
        "exp_2178"
      ],
      "cosine_similarity": 0.8428091349725318,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.48,
      "score_detection": 0.42857142857142855,
      "score_simulation": null,
      "score_embedding": 0.245625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.39,
      "score_detection": 0.42,
      "score_simulation": null,
      "score_embedding": 0.2192
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5142857142857142,
      "score_detection": 0.45714285714285713,
      "score_simulation": null,
      "score_embedding": 0.30625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.30625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4,
      "score_detection": 0.42857142857142855,
      "score_simulation": null,
      "score_embedding": 0.2192
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.375,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.245625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.34,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.2192
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.38,
      "score_detection": 0.46,
      "score_simulation": null,
      "score_embedding": 0.245625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.30625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.4052796714457685,
      "z_score_fuzz": -1.6185454008717302,
      "z_score_detection": -0.9079938786897612
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.5515649687278914,
      "z_score_fuzz": -1.8586114833849614,
      "z_score_detection": -1.0016031734088828
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.0696677074825245,
      "z_score_fuzz": -0.6370507369911024,
      "z_score_detection": -0.5720033744300542
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.31060631700242
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.4705932085072453
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.7595739396345603
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}