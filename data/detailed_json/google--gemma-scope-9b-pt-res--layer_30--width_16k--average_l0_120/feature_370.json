{
  "feature_id": 370,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1133",
      "text": "URLs, website addresses, and domain names, often including top-level domains and subdomains, as well as specific web-related terms and protocols.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_309",
      "text": "Patterns involving domain names, URLs, and web-related identifiers, including subdomains, top-level domains (like .uk, .org, .co), protocol prefixes (http://, https://), and common web structures (www, @, .com, .net), often appearing in contexts related to online resources, links, or digital infrastructure.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1956",
      "text": "The highlighted tokens are the core lexical pieces that give a phrase or a URL its meaningâ€”typically nouns, adjectives, or key prepositions that form idiomatic expressions or domain identifiers. They usually appear surrounded by spaces or punctuation and serve as the building blocks of the phrase or web address.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1133",
        "exp_309"
      ],
      "cosine_distance": 0.06611477599482829,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1133",
        "exp_1956"
      ],
      "cosine_distance": 0.149216809827467,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_309",
        "exp_1956"
      ],
      "cosine_distance": 0.1411991197677831,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.76,
      "score_detection": 0.78,
      "score_simulation": 0.25296189101908206,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.475,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8166666666666667,
      "score_detection": 0.8,
      "score_simulation": 0.26753530357769073,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.8,
      "score_detection": 0.86,
      "score_simulation": 0.20129048964466134,
      "score_embedding": 0.7444
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.77,
      "score_detection": 0.82,
      "score_simulation": 0.14177768910691121,
      "score_embedding": 0.79
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}