{
  "feature_id": 550,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1274",
      "text": "Various programming-related terms, including class names, method names, and keywords, often used in the context of caching, memory management, and data structures.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_450",
      "text": "Tokens related to caching mechanisms, data storage, or memory management in code, often appearing as identifiers, method names, or configuration keys, with frequent use of compound terms like \\\"cache\\\", \\\"cacheKey\\\", \\\"cached\\\", \\\"cacheService\\\", and similar patterns involving prefixes, suffixes, or camelCase naming.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2097",
      "text": "The highlighted tokens are identifiers or keywords that signal a caching concept—such as “Cache”, “cache”, “cache:”, “CacheKey”, “cacheGroups”, “cacheObserver”, “cacheService”, etc.—used throughout code to refer to cache objects, cache settings, or cache‑related operations.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1274",
        "exp_450"
      ],
      "cosine_distance": 0.07512413565565879,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1274",
        "exp_2097"
      ],
      "cosine_distance": 0.08677083180692569,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_450",
        "exp_2097"
      ],
      "cosine_distance": 0.058527792294204684,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6571428571428571,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.63,
      "score_detection": 0.57,
      "score_simulation": 0.0878028504203202,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.5066666666666667,
      "score_simulation": 0.1248151329840337,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.75,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.66,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.66,
      "score_detection": 0.64,
      "score_simulation": 0.358024174967218,
      "score_embedding": 0.5687999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}