{
  "feature_id": 116,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_919",
      "text": "Chemical and biological terms, often representing enzymes, acids, proteins, and other compounds, typically denoted by their scientific names or abbreviations.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_095",
      "text": "Shortened or truncated forms of biochemical and molecular biology terms, often appearing as partial words or fragments, are frequently activated in scientific text, particularly when representing complex compound names, enzyme names, or metabolic pathways.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1743",
      "text": "The activations consistently highlight subword fragments that belong to technical terms—often chemical or biological names—capturing prefixes, suffixes, or internal substrings, sometimes with preceding spaces or punctuation. These fragments are the building blocks of longer domain‑specific words, indicating the model’s focus on the compositional structure of scientific terminology.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_919",
        "exp_095"
      ],
      "cosine_similarity": 0.9010716055122693,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_919",
        "exp_1743"
      ],
      "cosine_similarity": 0.8756360663028865,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_095",
        "exp_1743"
      ],
      "cosine_similarity": 0.8932970554425526,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.44960000000000006
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.79,
      "score_detection": 0.51,
      "score_simulation": 0.20031773314967577,
      "score_embedding": 0.8212
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.475,
      "score_simulation": 0.2640595248781227,
      "score_embedding": 0.8212
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.44960000000000006
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.81,
      "score_detection": 0.49,
      "score_simulation": 0.23111590628043194,
      "score_embedding": 0.8212
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.65,
      "score_detection": 0.51,
      "score_simulation": 0.3134964560323063,
      "score_embedding": 0.44960000000000006
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}