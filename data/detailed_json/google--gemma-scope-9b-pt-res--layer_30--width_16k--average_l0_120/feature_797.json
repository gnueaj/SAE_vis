{
  "feature_id": 797,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1482",
      "text": "Prepositions, particularly \\\"to\\\", often used in infinitive phrases, and other function words that provide context and facilitate connections between clauses or phrases.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_658",
      "text": "The word \\\"to\\\" frequently appears in infinitive verb constructions, often introducing purpose or intention, and is commonly used in formal or technical writing to link actions to their goals or outcomes.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2305",
      "text": "Tokens that act as functional connectors indicating purpose or relation, such as prepositions and infinitive markers (“to”, “for”) and related verbs (“used”, “support”, “aid”, “help”, “guide”, “permit”, “relied”, “basis”, “pat”, “prognosis”).",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1482",
        "exp_658"
      ],
      "cosine_similarity": 0.8981490386554805,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1482",
        "exp_2305"
      ],
      "cosine_similarity": 0.9114951890145159,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_658",
        "exp_2305"
      ],
      "cosine_similarity": 0.8694527591149693,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.688125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8,
      "score_detection": 0.53,
      "score_simulation": null,
      "score_embedding": 0.6768
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6571428571428571,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.6125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.85,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.6125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.775,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.6768
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.688125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.77,
      "score_detection": 0.64,
      "score_simulation": null,
      "score_embedding": 0.6768
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.875,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.688125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.825,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.6125
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}