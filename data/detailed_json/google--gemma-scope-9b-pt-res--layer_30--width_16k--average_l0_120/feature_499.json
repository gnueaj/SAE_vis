{
  "feature_id": 499,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1235",
      "text": "Verbs or verb phrases indicating actions, states, or events that have occurred or will occur, often in relation to the subject's past or future experiences, and sometimes used in idiomatic expressions or to convey a sense of change or progression.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_411",
      "text": "Common phrasal constructions involving temporal or modal expressions, often indicating past experiences, habitual actions, or hypothetical conditions, with high activation on function words like \\\"to\\\", \\\"never\\\", \\\"before\\\", \\\"ever\\\", \\\"up\\\", \\\"out\\\", and \\\"at\\\", and on key content words that complete idiomatic or grammatical structures.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2058",
      "text": "The highlighted words are short function tokens that frequently appear in common collocations expressing time, experience, or direction (e.g., “never before,” “ever before,” “to find,” “to return,” “to see”). They serve as connectors that frame the surrounding content, indicating temporal or experiential context.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1235",
        "exp_411"
      ],
      "cosine_similarity": 0.8797791616401762,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1235",
        "exp_2058"
      ],
      "cosine_similarity": 0.8462738498005462,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_411",
        "exp_2058"
      ],
      "cosine_similarity": 0.8843107135710193,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.685625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.73,
      "score_detection": 0.54,
      "score_simulation": null,
      "score_embedding": 0.4964
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.36666666666666664,
      "score_simulation": null,
      "score_embedding": 0.648125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.575,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.648125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.55,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.4964
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": 0.685625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6842105263157895,
      "score_detection": 0.57,
      "score_simulation": null,
      "score_embedding": 0.4964
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.78,
      "score_detection": 0.66,
      "score_simulation": null,
      "score_embedding": 0.685625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": 0.648125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.0305020051534433,
      "z_score_fuzz": 0.42773215959819827,
      "z_score_detection": 0.4877875693542942
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.01702251247038824,
      "z_score_fuzz": 0.04868045036677997,
      "z_score_detection": -0.1557763318396719
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.8229069758978281,
      "z_score_fuzz": -0.4410784247354021,
      "z_score_detection": -0.2571864011187205
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.6486739113686452
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.04137279798109339
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.041547383347901846
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}