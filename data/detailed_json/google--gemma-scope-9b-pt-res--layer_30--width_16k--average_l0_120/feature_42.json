{
  "feature_id": 42,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_859",
      "text": "Words that start with \\\"ba\\\", \\\"be\\\", or \\\"bad\\\" often representing nouns or adjectives, sometimes related to concepts of conflict, geography, or objects.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_035",
      "text": "Fragments of proper nouns, common words, or morphological variants (e.g., \\\"ba\\\", \\\"bag\\\", \\\"bank\\\", \\\"bad\\\", \\\"battle\\\") that appear in contexts involving names, places, or compound terms, often preceding or following punctuation or being part of a larger lexical unit.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1683",
      "text": "The salient tokens are subword fragments that appear inside larger words—often prefixes, suffixes, or internal pieces—indicating that the model’s behavior is largely driven by subword‑level lexical patterns rather than whole‑word tokens.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_859",
        "exp_035"
      ],
      "cosine_similarity": 0.887392763865948,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_859",
        "exp_1683"
      ],
      "cosine_similarity": 0.7979827034569026,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_035",
        "exp_1683"
      ],
      "cosine_similarity": 0.8794194889867561,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.475,
      "score_detection": 0.26666666666666666,
      "score_simulation": null,
      "score_embedding": 0.683125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.7,
      "score_detection": 0.67,
      "score_simulation": null,
      "score_embedding": 0.6392
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5333333333333333,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": 0.690625
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.690625
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.825,
      "score_detection": 0.7142857142857143,
      "score_simulation": null,
      "score_embedding": 0.6392
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.683125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.92,
      "score_detection": 0.64,
      "score_simulation": null,
      "score_embedding": 0.6392
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.63,
      "score_detection": 0.27,
      "score_simulation": null,
      "score_embedding": 0.683125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.6,
      "score_simulation": null,
      "score_embedding": 0.690625
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.0166623365364018,
      "z_score_fuzz": -0.4296467065204868,
      "z_score_detection": -1.6145211745459929
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.773499358934992,
      "z_score_fuzz": 1.1479304071378935,
      "z_score_detection": 0.8137485063226658
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.0581813423875253,
      "z_score_fuzz": -0.25054978782013865,
      "z_score_detection": -0.07888298260610735
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.3425018481766926
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.9117260907985171
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.24291619065375977
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}