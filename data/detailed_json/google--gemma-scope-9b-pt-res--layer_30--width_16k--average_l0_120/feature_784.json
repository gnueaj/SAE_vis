{
  "feature_id": 784,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1473",
      "text": "Prepositions and articles often precede nouns, especially those referring to locations, organizations, and events, while some nouns are part of proper nouns or names of specific places, teams, or brands.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_649",
      "text": "Common patterns include location-specific nouns (e.g., stadiums, cities, landmarks) and prepositional phrases (e.g., \\\"at\\\", \\\"in\\\", \\\"of the\\\") that denote spatial or contextual relationships, often appearing in proximity to proper nouns or geographical references.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2296",
      "text": "The highlighted tokens are usually short function words or fragments of multiâ€‘word proper nouns that serve as connectors or identifiers in phrases, especially in location or organization names, often appearing in contiguous sequences that form a phrase.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1473",
        "exp_649"
      ],
      "cosine_similarity": 0.9091533765620954,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1473",
        "exp_2296"
      ],
      "cosine_similarity": 0.8630765703516877,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_649",
        "exp_2296"
      ],
      "cosine_similarity": 0.8756351296712555,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.5142857142857142,
      "score_simulation": null,
      "score_embedding": 0.661875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.64,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.6275999999999999
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5666666666666667,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.58125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.58125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.4666666666666667,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.6275999999999999
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.725,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.661875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.69,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": 0.6275999999999999
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.661875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.55,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.58125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.8990251532915536,
      "z_score_fuzz": 0.5306176235324406,
      "z_score_detection": -0.43827581054559395
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.7092832965519216,
      "z_score_fuzz": -0.3343823880628547,
      "z_score_detection": -0.43660421599703847
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.4526958403919826,
      "z_score_fuzz": -0.34581410627777076,
      "z_score_detection": 0.4058794364750619
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.3304556554261335
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.020567769169323864
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.17092039019642458
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}