{
  "feature_id": 31,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_851",
      "text": "Numerical values, often decimal numbers, and sometimes hexadecimal values, that appear to be part of mathematical expressions, data, or measurements.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_027",
      "text": "Individual digits and punctuation marks within numerical or symbolic sequences, particularly in mathematical expressions, timestamps, or formatted data, often carry significant activation, with digits like '8', '9', '6', and '.' being frequently highlighted in contextually precise positions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1675",
      "text": "The highlighted tokens are numeric characters or symbols that form numeric literals (decimal, hexadecimal, scientific notation, etc.), often appearing in sequences that represent numbers.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_851",
        "exp_027"
      ],
      "cosine_similarity": 0.8750619782817212,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_851",
        "exp_1675"
      ],
      "cosine_similarity": 0.9168121636194333,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_027",
        "exp_1675"
      ],
      "cosine_similarity": 0.9043025535643621,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.75,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.5750000000000001
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.76,
      "score_detection": 0.61,
      "score_simulation": null,
      "score_embedding": 0.6756
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": 0.586875
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.725,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.586875
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": 0.6756
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.5750000000000001
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.67,
      "score_simulation": null,
      "score_embedding": 0.6756
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.84,
      "score_detection": 0.71,
      "score_simulation": null,
      "score_embedding": 0.5750000000000001
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.725,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.586875
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.4180966688493803,
      "z_score_fuzz": 0.9650229156992404,
      "z_score_detection": 0.8388224245510021
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.9750049339991086,
      "z_score_fuzz": 0.6106396510368515,
      "z_score_detection": 0.534592216713854
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.4838350947803247,
      "z_score_fuzz": 0.4734590324578618,
      "z_score_detection": 0.6984084824723186
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.7406473363665409
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.7067456005832713
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.5519008699035016
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}