{
  "feature_id": 443,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1187",
      "text": "Punctuation marks, particularly periods and commas, often used to end sentences or separate items in lists, and sometimes category labels.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_363",
      "text": "Terminal punctuation marks (like periods and commas) and adjacent content often signal the end of a descriptive clause or list, particularly in travel or review-style text, with high activation on closing punctuation and surrounding words.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2010",
      "text": "The highlighted tokens are mainly punctuation marks and very common stop‑words (e.g., “.”, “,”, “and”, “the”, “Category”) that act as structural glue in the text, marking phrase or sentence boundaries and linking clauses.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1187",
        "exp_363"
      ],
      "cosine_similarity": 0.9015220352067571,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1187",
        "exp_2010"
      ],
      "cosine_similarity": 0.8789492098965548,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_363",
        "exp_2010"
      ],
      "cosine_similarity": 0.873818377675783,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6857142857142857,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.36875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.67,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.4956
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": 0.52
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.52
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.4956
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.625,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.36875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.81,
      "score_detection": 0.46,
      "score_simulation": null,
      "score_embedding": 0.4956
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.78,
      "score_detection": 0.48,
      "score_simulation": null,
      "score_embedding": 0.36875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.52
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.7236759920565002,
      "z_score_fuzz": 0.33791151648100304,
      "z_score_detection": -0.811041394873527
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.021451206427841474,
      "z_score_fuzz": 0.2562563863744619,
      "z_score_detection": -0.35469608311780654
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.1136239592744786,
      "z_score_fuzz": 0.5306176235324406,
      "z_score_detection": -0.4884236470022668
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.39893529014967477
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.039963634390395374
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.05193931193488414
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}