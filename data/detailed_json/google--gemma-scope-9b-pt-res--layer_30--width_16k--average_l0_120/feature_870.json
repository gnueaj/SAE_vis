{
  "feature_id": 870,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1536",
      "text": "Punctuation marks, conjunctions, and prepositions that connect clauses or phrases, often indicating a transition or a relationship between ideas.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_712",
      "text": "Punctuation marks, conjunctions, and prepositions that connect clauses or phrases, often indicating a transition or a relationship between ideas.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2359",
      "text": "The activations consistently target high‑frequency function words and simple lexical items—such as “the,” “to,” “like,” “small,” and comparative endings like “‑er”—that appear in common syntactic patterns (prepositional phrases, simple adjectives, or comparative constructions). This indicates the model’s focus on these ubiquitous tokens for general language processing.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1536",
        "exp_712"
      ],
      "cosine_distance": 0.0,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1536",
        "exp_2359"
      ],
      "cosine_distance": 0.18427877242047797,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_712",
        "exp_2359"
      ],
      "cosine_distance": 0.18427877242047797,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.43333333333333335,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.53,
      "score_detection": 0.56,
      "score_simulation": 0.009260135904398772,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5,
      "score_detection": 0.5076923076923077,
      "score_simulation": -0.01175877659695701,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.5,
      "score_detection": 0.625,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.52,
      "score_detection": 0.61,
      "score_simulation": -0.03262438054628754,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.475,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.475,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}