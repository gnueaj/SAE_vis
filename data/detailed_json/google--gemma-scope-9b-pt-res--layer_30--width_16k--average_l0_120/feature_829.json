{
  "feature_id": 829,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1508",
      "text": "Verbs or adjectives describing or modifying a noun, often indicating a change of state, appearance, or condition.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_684",
      "text": "Commonly activated tokens include function words (e.g., \\\"the\\\", \\\"of\\\", \\\"on\\\", \\\"in\\\", \\\"up\\\") and derivational suffixes (e.g., \\\"-ed\\\", \\\"-ing\\\", \\\"-er\\\", \\\"-ly\\\", \\\"-ized\\\", \\\"-ated\\\") that modify or connect words in syntactic or semantic roles. These tokens often appear in contextually rich phrases involving physical attributes, spatial relations, or grammatical constructions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2331",
      "text": "The highlighted tokens are the core lexical items that carry the main meaning of a phraseâ€”typically nouns, adjectives, participles, or the prepositions that link them. They serve as heads or modifiers in noun or adjective phrases, providing the semantic weight that the model deems most important.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1508",
        "exp_684"
      ],
      "cosine_similarity": 0.8366533097424755,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1508",
        "exp_2331"
      ],
      "cosine_similarity": 0.8379891432264832,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_684",
        "exp_2331"
      ],
      "cosine_similarity": 0.880581817073652,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5142857142857142,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.5574999999999999
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.5716
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.5237499999999999
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.525,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.5237499999999999
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.625,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.5716
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.725,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.5574999999999999
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.63,
      "score_detection": 0.39,
      "score_simulation": null,
      "score_embedding": 0.5716
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.625,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.5574999999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.5,
      "score_detection": 0.35,
      "score_simulation": null,
      "score_embedding": 0.5237499999999999
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.32121898853009245,
      "z_score_fuzz": -0.17978200839446967,
      "z_score_detection": -0.5887193199156117
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.39927471953020416,
      "z_score_fuzz": -0.20101234222217082,
      "z_score_detection": -0.7525355856740759
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.1343834622000396,
      "z_score_fuzz": -0.7268713801082977,
      "z_score_detection": -0.7057309383145148
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.1490941132599963
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.18475773612201418
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.4327396187409243
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}