{
  "feature_id": 417,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1167",
      "text": "Mathematical expressions, equations, and operations, often involving variables, constants, and arithmetic operators, and sometimes including function definitions, derivatives, and simplifications.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_343",
      "text": "Mathematical operations and symbols, particularly those involving comparison, arithmetic, and symbolic notation, are frequently highlighted in contexts involving algebraic expressions, equations, and formal mathematical reasoning.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1990",
      "text": "The highlighted tokens are typically short, high‑frequency function words or operators that act as syntactic glue in mathematical or programming expressions, often forming part of multi‑word phrases such as “divided by”, “multiplying”, “union all”, “postfix”, “Suppose”, “Closing”, “Minus”, “Plus sign”, “derivative”, etc. They are not content words but structural elements that convey the core meaning of the phrase.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1167",
        "exp_343"
      ],
      "cosine_similarity": 0.9122999329734803,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1167",
        "exp_1990"
      ],
      "cosine_similarity": 0.8388388605377232,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_343",
        "exp_1990"
      ],
      "cosine_similarity": 0.851677770613701,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.37142857142857144,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.384375
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.47,
      "score_detection": 0.43,
      "score_simulation": null,
      "score_embedding": 0.5044000000000001
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.675,
      "score_detection": 0.4666666666666667,
      "score_simulation": null,
      "score_embedding": 0.19187500000000002
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.75,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.19187500000000002
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.3142857142857143,
      "score_detection": 0.275,
      "score_simulation": null,
      "score_embedding": 0.5044000000000001
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.4,
      "score_simulation": null,
      "score_embedding": 0.384375
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.27,
      "score_detection": 0.38,
      "score_simulation": null,
      "score_embedding": 0.5044000000000001
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.46,
      "score_detection": 0.43,
      "score_simulation": null,
      "score_embedding": 0.384375
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": 0.19187500000000002
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.6371780631999941,
      "z_score_fuzz": -1.7410280960315418,
      "z_score_detection": -1.103570440870784
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.027264427104143223,
      "z_score_fuzz": -2.0317203592108295,
      "z_score_detection": -1.384398325028151
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.702832546712149,
      "z_score_fuzz": 0.416300441383283,
      "z_score_detection": -0.08166897352036692
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.1605922000341067
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.1296180857116125
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.45606702628307766
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}