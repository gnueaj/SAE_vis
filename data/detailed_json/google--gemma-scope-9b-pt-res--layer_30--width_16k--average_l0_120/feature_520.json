{
  "feature_id": 520,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1249",
      "text": "A single character, often \\\"Q\\\", used as a variable or identifier in various contexts, including programming, mathematics, and scientific notation.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_425",
      "text": "The token \\\"Q\\\" and similar symbols (like \\\"q\\\", \\\"X\\\", \\\"Y\\\", \\\"PQ\\\", \\\"0\\\", \\\"/\\\") frequently appear in technical, programming, or mathematical contexts, often as placeholders, variable names, or syntax markers in code, equations, or configuration files. Their activation patterns suggest they are used to denote abstract or symbolic elements in structured text, particularly in code snippets, mathematical expressions, or system configurations.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2072",
      "text": "The highlighted items are short, non‑lexical symbols—single letters or brief abbreviations—that act as placeholders or variable names in code, equations, or technical prose, and thus carry high semantic weight in the surrounding context.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1249",
        "exp_425"
      ],
      "cosine_similarity": 0.896186311763749,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1249",
        "exp_2072"
      ],
      "cosine_similarity": 0.8547743452902854,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_425",
        "exp_2072"
      ],
      "cosine_similarity": 0.9030089087429192,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5428571428571428,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.330625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.75,
      "score_detection": 0.49,
      "score_simulation": null,
      "score_embedding": 0.3156
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8285714285714286,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": 0.391875
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.391875
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.3156
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.8,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.330625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.7684210526315789,
      "score_detection": 0.53,
      "score_simulation": null,
      "score_embedding": 0.3156
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.68,
      "score_detection": 0.42,
      "score_simulation": null,
      "score_embedding": 0.330625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.8571428571428571,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.391875
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.9347309384663752,
      "z_score_fuzz": 0.18276676927857485,
      "z_score_detection": -0.7759379093538565
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.0179073468547915,
      "z_score_fuzz": 0.6870516622628671,
      "z_score_detection": -0.015362389760987673
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.5956590573488715,
      "z_score_fuzz": 1.1838586643847728,
      "z_score_detection": -0.2544004102044609
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.5093006928472189
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.11540602478430402
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.11126639894381347
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}