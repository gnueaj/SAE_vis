{
  "feature_id": 182,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_978",
      "text": "The word \\\"which\\\" is often used in comparative or interrogative contexts, typically to inquire about or distinguish between quantities, values, or options.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_154",
      "text": "The word \\\"Which\\\" is frequently used in comparative questions involving numerical or quantitative values, often followed by \\\"is\\\" and a comparison operator, with high activation on \\\"Which\\\" and \\\"is\\\" when evaluating magnitude or proximity.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1802",
      "text": "The highlighted tokens are usually key words that anchor the sentence—often interrogative pronouns like “Which” or “which” or specific nouns such as “Royal” or objects/places that the sentence centers on.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_978",
        "exp_154"
      ],
      "cosine_similarity": 0.9287632070218926,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_978",
        "exp_1802"
      ],
      "cosine_similarity": 0.8520488990615428,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_154",
        "exp_1802"
      ],
      "cosine_similarity": 0.8450685231814844,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.525,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.15125
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.78,
      "score_detection": 0.56,
      "score_simulation": null,
      "score_embedding": 0.23
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.725,
      "score_detection": 0.45,
      "score_simulation": null,
      "score_embedding": 0.306875
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.306875
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.525,
      "score_simulation": null,
      "score_embedding": 0.23
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.575,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.15125
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.69,
      "score_detection": 0.59,
      "score_simulation": null,
      "score_embedding": 0.23
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.59,
      "score_detection": 0.54,
      "score_simulation": null,
      "score_embedding": 0.15125
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.6,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.306875
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -1.927727161739065,
      "z_score_fuzz": -0.5782590433143918,
      "z_score_detection": -0.5536158343959412
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -1.4917776003022745,
      "z_score_fuzz": 0.23339294994462975,
      "z_score_detection": -0.003661227921098684
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -1.0662077903282643,
      "z_score_fuzz": 0.35914185030870416,
      "z_score_detection": -0.764236747513966
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.0198673464831327
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.4206819594262478
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.4904342291778421
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}