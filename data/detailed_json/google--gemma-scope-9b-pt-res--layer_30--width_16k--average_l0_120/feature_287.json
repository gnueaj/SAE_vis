{
  "feature_id": 287,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1065",
      "text": "Prepositions and conjunctions, often used to connect words, phrases, or clauses in a sentence, and mathematical or programming operators and symbols.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_241",
      "text": "Common syntactic and semantic patterns involving comparative, relational, or conditional constructs in code and technical text, often centered around operators, logical conditions, or structural markers like \\\"if\\\", \\\"to\\\", \\\"of\\\", \\\"than\\\", \\\"that\\\", and punctuation sequences.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1888",
      "text": "The highlighted tokens are the short, high‑frequency function words and code keywords that act as the grammatical or syntactic glue in a sentence or program. They include prepositions, conjunctions, articles, and control‑flow operators, as well as punctuation that signals a clause boundary.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1065",
        "exp_241"
      ],
      "cosine_similarity": 0.8799764275657703,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1065",
        "exp_1888"
      ],
      "cosine_similarity": 0.8832881324256027,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_241",
        "exp_1888"
      ],
      "cosine_similarity": 0.871217792829887,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.5428571428571428,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.42,
      "score_detection": 0.51,
      "score_simulation": 0.10135472781444567,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.45,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 1.0,
      "score_detection": 0.45,
      "score_simulation": 0.1419133108787146,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.55,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.51,
      "score_detection": 0.35,
      "score_simulation": 0.14281047920788373,
      "score_embedding": 0.618
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.48,
      "score_detection": 0.48,
      "score_simulation": 0.29498734263979226,
      "score_embedding": 0.47959999999999997
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}