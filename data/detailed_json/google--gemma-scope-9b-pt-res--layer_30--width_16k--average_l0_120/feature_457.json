{
  "feature_id": 457,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1200",
      "text": "Numerical digits embedded within text, often as part of a larger numerical value or identifier.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_376",
      "text": "Individual digits in numerical sequences, particularly those forming part of dates, measurements, or identifiers, with higher activation values when they appear in contextually significant positions such as years, decimal points, or large numbers.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2023",
      "text": "The highlighted tokens are single characters that belong to numeric literals or short alphanumeric identifiers, typically digits within numbers or short codes.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1200",
        "exp_376"
      ],
      "cosine_similarity": 0.906011376020436,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1200",
        "exp_2023"
      ],
      "cosine_similarity": 0.9066602438057081,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_376",
        "exp_2023"
      ],
      "cosine_similarity": 0.88731752757328,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6285714285714286,
      "score_detection": 0.55,
      "score_simulation": null,
      "score_embedding": 0.8325
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.52,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.5764
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8333333333333334,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": 0.819375
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.65,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.819375
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.575,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.5764
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.8325
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.68,
      "score_detection": 0.44,
      "score_simulation": null,
      "score_embedding": 0.5764
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.77,
      "score_detection": 0.43,
      "score_simulation": null,
      "score_embedding": 0.8325
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.675,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.819375
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 1.8435825364046001,
      "z_score_fuzz": 0.12724128080612626,
      "z_score_detection": -0.46000653967681876
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 0.425846883274923,
      "z_score_fuzz": -0.3839198336608233,
      "z_score_detection": -0.6238228054352829
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.770924276165135,
      "z_score_fuzz": 0.49251189614938806,
      "z_score_detection": -0.6639410746006211
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.5036057591779692
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.1939652519403944
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.5331650325713007
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}