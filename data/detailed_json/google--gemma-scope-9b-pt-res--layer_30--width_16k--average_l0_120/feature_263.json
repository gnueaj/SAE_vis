{
  "feature_id": 263,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1045",
      "text": "Prepositions \\\"to\\\" and \\\"for\\\" often used to indicate purpose, necessity, or direction, frequently preceding verbs or nouns in formal or technical contexts.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_221",
      "text": "The word \\\"to\\\" frequently appears in infinitive verb constructions, often introducing purpose or necessity, and is highly activated in contexts involving goals, requirements, or conditions. The word \\\"for\\\" also appears in similar functional roles, particularly in expressions of purpose or necessity, though less frequently than \\\"to\\\".",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1868",
      "text": "The text repeatedly highlights the preposition “to” (and sometimes “for”) as a highly activated, function‑word connector that introduces infinitives or prepositional phrases, signaling purpose or direction in the sentence structure.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1045",
        "exp_221"
      ],
      "cosine_similarity": 0.9155091841749529,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1045",
        "exp_1868"
      ],
      "cosine_similarity": 0.8952031239619852,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_221",
        "exp_1868"
      ],
      "cosine_similarity": 0.9134294064436839,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.7428571428571429,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.625625
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.88,
      "score_detection": 0.61,
      "score_simulation": null,
      "score_embedding": 0.7179999999999999
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.925,
      "score_detection": 0.5428571428571428,
      "score_simulation": null,
      "score_embedding": 0.603125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.75,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.603125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.775,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": 0.7179999999999999
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.85,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.625625
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.86,
      "score_detection": 0.62,
      "score_simulation": null,
      "score_embedding": 0.7179999999999999
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.85,
      "score_detection": 0.62,
      "score_simulation": null,
      "score_embedding": 0.625625
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.85,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.603125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.6983499583444596,
      "z_score_fuzz": 1.1430310993315023,
      "z_score_detection": 0.45268408383462333
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 1.2097257137441229,
      "z_score_fuzz": 1.3079744621467155,
      "z_score_detection": 0.4760864075144029
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 0.573792940791091,
      "z_score_fuzz": 1.3308378985765468,
      "z_score_detection": -0.13738879180555852
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.7646883805035284
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 0.9979288611350805
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.589080682520693
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}