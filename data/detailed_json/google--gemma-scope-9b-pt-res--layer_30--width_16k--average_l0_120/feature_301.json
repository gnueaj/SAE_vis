{
  "feature_id": 301,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1077",
      "text": "Punctuation marks and conjunctions that connect clauses or phrases, often used to introduce contrasting or additional information.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_253",
      "text": "The tokens \\\"and\\\", \\\"but\\\", \\\",\\\", \\\".\\\", and \\\"not\\\" are frequently activated in contexts involving logical or contrastive conjunctions, punctuation marking sentence boundaries, or transitions between clauses, often signaling relationships such as addition, contrast, or temporal sequence.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1900",
      "text": "The model identifies coordinating conjunctions and punctuation as key tokens that mark syntactic or discourse connections between clauses.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_distance_pairs": [
    {
      "pair": [
        "exp_1077",
        "exp_253"
      ],
      "cosine_distance": 0.0946412455584924,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_1077",
        "exp_1900"
      ],
      "cosine_distance": 0.1102732032127095,
      "euclidean_distance": null
    },
    {
      "pair": [
        "exp_253",
        "exp_1900"
      ],
      "cosine_distance": 0.10452749994705857,
      "euclidean_distance": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8285714285714286,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.76,
      "score_detection": 0.59,
      "score_simulation": 0.22915028576001992,
      "score_embedding": null
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.875,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.6,
      "score_simulation": 0.4024428611894072,
      "score_embedding": null
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.85,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": null
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.83,
      "score_detection": 0.68,
      "score_simulation": 0.3481240241526963,
      "score_embedding": 0.4244
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.79,
      "score_detection": 0.7,
      "score_simulation": 0.22163040024246042,
      "score_embedding": 0.46120000000000005
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.925,
      "score_detection": 0.65,
      "score_simulation": null,
      "score_embedding": null
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}