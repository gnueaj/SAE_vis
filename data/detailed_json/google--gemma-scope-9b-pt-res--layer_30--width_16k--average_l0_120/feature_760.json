{
  "feature_id": 760,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1454",
      "text": "Specialized or technical terms, often denoted as proper nouns, names of concepts, or specific entities, and sometimes mathematical or scientific expressions.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_630",
      "text": "The text latents consistently involve technical, academic, or formal language with frequent use of specialized terminology, proper nouns, numerical data, and structured formatting (e.g., citations, mathematical notation, code syntax, or scientific notation), often within contexts involving research, documentation, or precise procedural descriptions.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2277",
      "text": "The highlighted portions consistently contain the core lexical items that carry the main meaning of each sentenceâ€”idiomatic phrases, comparative adjectives, or noun phrases that denote objects or concepts. These tokens receive higher activation scores, while surrounding function words or punctuation are typically omitted.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1454",
        "exp_630"
      ],
      "cosine_similarity": 0.8895178734563022,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1454",
        "exp_2277"
      ],
      "cosine_similarity": 0.827463533861378,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_630",
        "exp_2277"
      ],
      "cosine_similarity": 0.8266159802723702,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.425,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.46375000000000005
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.28,
      "score_detection": 0.28,
      "score_simulation": null,
      "score_embedding": 0.4256
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6285714285714286,
      "score_detection": 0.575,
      "score_simulation": null,
      "score_embedding": 0.36875
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.675,
      "score_detection": 0.8,
      "score_simulation": null,
      "score_embedding": 0.36875
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.45,
      "score_detection": 0.425,
      "score_simulation": null,
      "score_embedding": 0.4256
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.35,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.46375000000000005
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.2736842105263158,
      "score_detection": 0.29,
      "score_simulation": null,
      "score_embedding": 0.4256
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.3,
      "score_detection": 0.3,
      "score_simulation": null,
      "score_embedding": 0.46375000000000005
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.75,
      "score_detection": 0.825,
      "score_simulation": null,
      "score_embedding": 0.36875
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.19776858460894312,
      "z_score_fuzz": -1.9843603837490353,
      "z_score_detection": -1.4663064579073826
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.40896192770498874,
      "z_score_fuzz": -2.147412785656519,
      "z_score_detection": -1.5950192381461754
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.7236759920565002,
      "z_score_fuzz": 0.25299018117020067,
      "z_score_detection": 1.2249607652673815
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -1.2161451420884537
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -1.3837979838358943
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 0.25142498479369396
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}