{
  "feature_id": 981,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1630",
      "text": "Punctuation marks, particularly periods, commas, and occasionally other marks, that typically indicate the end of a sentence or separate items in a list.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_806",
      "text": "The presence of punctuation marks such as periods and commas, often surrounding or following named entities, quoted text, or clauses, with high activation values indicating structural or syntactic boundaries in the text.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_2453",
      "text": "The highlighted tokens are usually short function words or punctuation that mark clause boundaries or key phrase components—often the final “er” of a comparative adjective, a noun that names a contained object, or a punctuation mark that signals a pause or end of a sentence.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1630",
        "exp_806"
      ],
      "cosine_similarity": 0.8992153567862659,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1630",
        "exp_2453"
      ],
      "cosine_similarity": 0.8859068447807988,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_806",
        "exp_2453"
      ],
      "cosine_similarity": 0.896335002178223,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.431875
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.69,
      "score_detection": 0.51,
      "score_simulation": null,
      "score_embedding": 0.4035999999999999
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.6571428571428571,
      "score_detection": 0.5333333333333333,
      "score_simulation": null,
      "score_embedding": 0.47687499999999994
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.6,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.47687499999999994
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.5,
      "score_simulation": null,
      "score_embedding": 0.4035999999999999
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.475,
      "score_simulation": null,
      "score_embedding": 0.431875
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.82,
      "score_detection": 0.47,
      "score_simulation": null,
      "score_embedding": 0.4035999999999999
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.85,
      "score_detection": 0.375,
      "score_simulation": null,
      "score_embedding": 0.431875
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.775,
      "score_detection": 0.25,
      "score_simulation": null,
      "score_embedding": 0.47687499999999994
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": -0.37422435947621574,
      "z_score_fuzz": 0.7592519878307565,
      "z_score_detection": -0.764236747513966
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": -0.5307510115349497,
      "z_score_fuzz": 0.8392740153351674,
      "z_score_detection": -0.46000653967681876
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": -0.12511032436947864,
      "z_score_fuzz": 0.20399710310627447,
      "z_score_detection": -0.9202522387125033
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": -0.1264030397198084
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": -0.050494511958867026
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": -0.2804551533252358
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}