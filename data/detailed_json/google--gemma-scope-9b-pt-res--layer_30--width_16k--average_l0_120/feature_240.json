{
  "feature_id": 240,
  "sae_id": "google/gemma-scope-9b-pt-res/layer_30/width_16k/average_l0_120",
  "explanations": [
    {
      "explanation_id": "exp_1026",
      "text": "Titles or honorifics preceding names of individuals, often indicating positions of authority or prestige.",
      "explanation_method": "quantiles",
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "data_source": "llama_e-llama_s"
    },
    {
      "explanation_id": "exp_202",
      "text": "Titles or honorifics (e.g., Sir, Admiral, President, Secretary) are frequently followed by personal names, and the most activated tokens are typically the title components and the associated proper names, with the title often serving as a key identifier for formal or official roles.",
      "explanation_method": "quantiles",
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "data_source": "gwen_e-llama_s"
    },
    {
      "explanation_id": "exp_1849",
      "text": "The highlighted tokens are titles or honorifics that precede names or positions, often capitalized and sometimes multiâ€‘word, indicating rank, office, or formal address.",
      "explanation_method": "quantiles",
      "llm_explainer": "openai/gpt-oss-20b",
      "data_source": "openai_e-llama_s"
    }
  ],
  "semantic_similarity_pairs": [
    {
      "pair": [
        "exp_1026",
        "exp_202"
      ],
      "cosine_similarity": 0.9201018828875703,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_1026",
        "exp_1849"
      ],
      "cosine_similarity": 0.9407052362215135,
      "euclidean_similarity": null
    },
    {
      "pair": [
        "exp_202",
        "exp_1849"
      ],
      "cosine_similarity": 0.9284885667926784,
      "euclidean_similarity": null
    }
  ],
  "scores": [
    {
      "data_source": "gwen_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.65,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.6712499999999999
    },
    {
      "data_source": "llama_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.77,
      "score_detection": 0.69,
      "score_simulation": null,
      "score_embedding": 0.7924
    },
    {
      "data_source": "openai_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.69125
    },
    {
      "data_source": "openai_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.825,
      "score_detection": 0.725,
      "score_simulation": null,
      "score_embedding": 0.69125
    },
    {
      "data_source": "llama_e-openai_s",
      "llm_scorer": "openai/gpt-oss-20b",
      "score_fuzz": 0.8,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.7924
    },
    {
      "data_source": "gwen_e-gwen_s",
      "llm_scorer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "score_fuzz": 0.775,
      "score_detection": 0.675,
      "score_simulation": null,
      "score_embedding": 0.6712499999999999
    },
    {
      "data_source": "llama_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.79,
      "score_detection": 0.75,
      "score_simulation": null,
      "score_embedding": 0.7924
    },
    {
      "data_source": "gwen_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.81,
      "score_detection": 0.69,
      "score_simulation": null,
      "score_embedding": 0.6712499999999999
    },
    {
      "data_source": "openai_e-llama_s",
      "llm_scorer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "score_fuzz": 0.8,
      "score_detection": 0.7,
      "score_simulation": null,
      "score_embedding": 0.69125
    }
  ],
  "normalized_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "z_score_embedding": 0.9509239106054568,
      "z_score_fuzz": 0.6677982421114311,
      "z_score_detection": 0.9090293955903438
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "z_score_embedding": 1.6215942517872628,
      "z_score_fuzz": 0.9535911974843259,
      "z_score_detection": 1.0845468231886974
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "z_score_embedding": 1.0616412595417852,
      "z_score_fuzz": 1.1022035342782301,
      "z_score_detection": 1.0494433376690273
    }
  ],
  "overall_scores": [
    {
      "llm_explainer": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
      "overall_score": 0.8425838494357439
    },
    {
      "llm_explainer": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
      "overall_score": 1.219910757486762
    },
    {
      "llm_explainer": "openai/gpt-oss-20b",
      "overall_score": 1.0710960438296808
    }
  ],
  "activating_examples": "TODO: Not implemented yet"
}